2020-07-13 19:36:38,012 DEBUG za.co.absa.hyperdrive.trigger.scheduler.eventProcessor.EventProcessor [ScalaTest-main-running-EventProcessorTest] Processing events. Sensor id: 1. Events: List(0)
2020-07-13 19:36:38,022 DEBUG za.co.absa.hyperdrive.trigger.scheduler.eventProcessor.EventProcessor [ScalaTest-main-running-EventProcessorTest] Processing events. Sensor id: 1. Events: List(0)
2020-07-13 19:36:38,026 DEBUG za.co.absa.hyperdrive.trigger.scheduler.eventProcessor.EventProcessor [ScalaTest-main-running-EventProcessorTest] Processing events. Sensor id: 1. Events: List(0)
2020-07-13 19:36:38,111 INFO org.quartz.impl.StdSchedulerFactory [ScalaTest-main-running-TimeSensorIntegrationTest] Using default implementation for ThreadExecutor
2020-07-13 19:36:38,125 INFO org.quartz.core.SchedulerSignalerImpl [ScalaTest-main-running-TimeSensorIntegrationTest] Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
2020-07-13 19:36:38,125 INFO org.quartz.core.QuartzScheduler [ScalaTest-main-running-TimeSensorIntegrationTest] Quartz Scheduler v.2.3.2 created.
2020-07-13 19:36:38,126 INFO org.quartz.simpl.RAMJobStore [ScalaTest-main-running-TimeSensorIntegrationTest] RAMJobStore initialized.
2020-07-13 19:36:38,126 INFO org.quartz.core.QuartzScheduler [ScalaTest-main-running-TimeSensorIntegrationTest] Scheduler meta-data: Quartz Scheduler (v2.3.2) 'za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler' with instanceId 'NON_CLUSTERED'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzSchedulerThreadPool' - with 0 threads.
  Using job-store 'org.quartz.simpl.RAMJobStore' - which does not support persistence. and is not clustered.

2020-07-13 19:36:38,127 INFO org.quartz.impl.StdSchedulerFactory [ScalaTest-main-running-TimeSensorIntegrationTest] Quartz scheduler 'za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler' initialized from an externally provided properties instance.
2020-07-13 19:36:38,127 INFO org.quartz.impl.StdSchedulerFactory [ScalaTest-main-running-TimeSensorIntegrationTest] Quartz scheduler version: 2.3.2
2020-07-13 19:36:38,127 INFO za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzSchedulerManager$ [ScalaTest-main-running-TimeSensorIntegrationTest] Starting Quartz Scheduler za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler now
2020-07-13 19:36:38,127 INFO org.quartz.core.QuartzScheduler [ScalaTest-main-running-TimeSensorIntegrationTest] Scheduler za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler_$_NON_CLUSTERED started.
2020-07-13 19:36:38,128 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [ScalaTest-main-running-TimeSensorIntegrationTest] Processing events. Sensors: Set()
2020-07-13 19:36:38,152 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-1-thread-6] Processing events successful
2020-07-13 19:36:39,011 INFO za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzJob [TimeSensorQuartz-thread-1] Starting Time Sensor Quartz Job
2020-07-13 19:36:39,015 DEBUG za.co.absa.hyperdrive.trigger.scheduler.eventProcessor.EventProcessor [TimeSensorQuartz-thread-1] Processing events. Sensor id: 1. Events: List(0)
2020-07-13 19:36:42,006 INFO za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzJob [TimeSensorQuartz-thread-2] Starting Time Sensor Quartz Job
2020-07-13 19:36:42,009 DEBUG za.co.absa.hyperdrive.trigger.scheduler.eventProcessor.EventProcessor [TimeSensorQuartz-thread-2] Processing events. Sensor id: 1. Events: List(0)
2020-07-13 19:36:43,162 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [ScalaTest-main-running-TimeSensorIntegrationTest] Processing events. Sensors: Set(1)
2020-07-13 19:36:43,168 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-1-thread-9] Processing events successful
2020-07-13 19:36:43,190 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [ScalaTest-main-running-TimeSensorIntegrationTest] Processing events. Sensors: Set(1)
2020-07-13 19:36:43,199 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-1-thread-13] Processing events successful
2020-07-13 19:36:43,202 INFO za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzSchedulerManager$ [ScalaTest-main-running-TimeSensorIntegrationTest] Stopping Quartz Scheduler za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler now
2020-07-13 19:36:43,202 INFO org.quartz.core.QuartzScheduler [ScalaTest-main-running-TimeSensorIntegrationTest] Scheduler za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler_$_NON_CLUSTERED paused.
2020-07-13 19:36:43,202 INFO za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzSchedulerManager$ [ScalaTest-main-running-TimeSensorIntegrationTest] Stopped Quartz Scheduler za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler
2020-07-13 19:36:43,473 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.shell.ShellExecutor$ [Thread-5] echo
2020-07-13 19:38:17,321 INFO scala.App$class [main] Starting App.class on Seneles-MBP with PID 691 (/Users/sonboy/.m2/repository/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar started by sonboy in /Users/sonboy/AbsaOSS/hyperdrive-trigger)
2020-07-13 19:38:17,328 INFO scala.App$class [main] No active profile set, falling back to default profiles: default
2020-07-13 19:38:17,440 INFO org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext [main] Refreshing org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@6a03bcb1: startup date [Mon Jul 13 19:38:17 SAST 2020]; root of context hierarchy
2020-07-13 19:38:19,322 INFO org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor [main] JSR-330 'javax.inject.Inject' annotation found and supported for autowiring
2020-07-13 19:38:19,760 INFO org.springframework.boot.web.embedded.tomcat.TomcatWebServer [main] Tomcat initialized with port(s): 7123 (http)
2020-07-13 19:38:19,780 INFO org.apache.coyote.http11.Http11NioProtocol [main] Initializing ProtocolHandler ["http-nio-7123"]
2020-07-13 19:38:19,796 INFO org.apache.catalina.core.StandardService [main] Starting service [Tomcat]
2020-07-13 19:38:19,797 INFO org.apache.catalina.core.StandardEngine [main] Starting Servlet Engine: Apache Tomcat/8.5.28
2020-07-13 19:38:19,832 INFO org.apache.catalina.core.AprLifecycleListener [localhost-startStop-1] The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: [/Users/sonboy/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.]
2020-07-13 19:38:20,041 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [localhost-startStop-1] Initializing Spring embedded WebApplicationContext
2020-07-13 19:38:20,042 INFO org.springframework.web.context.ContextLoader [localhost-startStop-1] Root WebApplicationContext: initialization completed in 2605 ms
2020-07-13 19:38:21,102 INFO org.springframework.boot.web.servlet.FilterRegistrationBean [localhost-startStop-1] Mapping filter: 'characterEncodingFilter' to: [/*]
2020-07-13 19:38:21,102 INFO org.springframework.boot.web.servlet.FilterRegistrationBean [localhost-startStop-1] Mapping filter: 'hiddenHttpMethodFilter' to: [/*]
2020-07-13 19:38:21,103 INFO org.springframework.boot.web.servlet.FilterRegistrationBean [localhost-startStop-1] Mapping filter: 'httpPutFormContentFilter' to: [/*]
2020-07-13 19:38:21,103 INFO org.springframework.boot.web.servlet.FilterRegistrationBean [localhost-startStop-1] Mapping filter: 'requestContextFilter' to: [/*]
2020-07-13 19:38:21,103 INFO org.springframework.boot.web.servlet.DelegatingFilterProxyRegistrationBean [localhost-startStop-1] Mapping filter: 'springSecurityFilterChain' to: [/*]
2020-07-13 19:38:21,103 INFO org.springframework.boot.web.servlet.FilterRegistrationBean [localhost-startStop-1] Mapping filter: 'httpTraceFilter' to: [/*]
2020-07-13 19:38:21,104 INFO org.springframework.boot.web.servlet.FilterRegistrationBean [localhost-startStop-1] Mapping filter: 'webMvcMetricsFilter' to: [/*]
2020-07-13 19:38:21,104 INFO org.springframework.boot.web.servlet.ServletRegistrationBean [localhost-startStop-1] Servlet dispatcherServlet mapped to [/]
2020-07-13 19:38:22,531 INFO za.co.absa.hyperdrive.trigger.api.rest.WebSecurityConfig$$EnhancerBySpringCGLIB$$4ecdbd80 [main] Using inmemory authentication
2020-07-13 19:38:22,758 INFO org.springframework.security.web.DefaultSecurityFilterChain [main] Creating filter chain: org.springframework.security.web.util.matcher.AnyRequestMatcher@1, [org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter@15eb0ae9, org.springframework.security.web.context.SecurityContextPersistenceFilter@4397a639, org.springframework.security.web.header.HeaderWriterFilter@2aaf152b, org.springframework.security.web.csrf.CsrfFilter@625a4ff, org.springframework.security.web.authentication.logout.LogoutFilter@49d831c2, org.springframework.security.web.authentication.UsernamePasswordAuthenticationFilter@f324455, org.springframework.security.web.savedrequest.RequestCacheAwareFilter@77114efe, org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter@56399b9e, org.springframework.security.web.authentication.AnonymousAuthenticationFilter@65e0b505, org.springframework.security.web.session.SessionManagementFilter@27fe9713, org.springframework.security.web.access.ExceptionTranslationFilter@1df1ced0, org.springframework.security.web.access.intercept.FilterSecurityInterceptor@3f183caa]
2020-07-13 19:38:22,785 INFO org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor [main] Initializing ExecutorService 
2020-07-13 19:38:22,795 INFO org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor [main] Initializing ExecutorService  'asyncExecutor'
2020-07-13 19:38:23,222 INFO org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter [main] Looking for @ControllerAdvice: org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@6a03bcb1: startup date [Mon Jul 13 19:38:17 SAST 2020]; root of context hierarchy
2020-07-13 19:38:23,341 INFO org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping [main] Mapped "{[/admin/isManager],methods=[GET]}" onto public java.util.concurrent.CompletableFuture<java.lang.Object> za.co.absa.hyperdrive.trigger.api.rest.controllers.AdminController.isManagerRunning()
2020-07-13 19:38:23,342 INFO org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping [main] Mapped "{[/admin/stopManager],methods=[POST]}" onto public java.util.concurrent.CompletableFuture<java.lang.Object> za.co.absa.hyperdrive.trigger.api.rest.controllers.AdminController.stopManager()
2020-07-13 19:38:23,343 INFO org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping [main] Mapped "{[/admin/startManager],methods=[POST]}" onto public java.util.concurrent.CompletableFuture<java.lang.Object> za.co.absa.hyperdrive.trigger.api.rest.controllers.AdminController.startManager()
2020-07-13 19:38:23,344 INFO org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping [main] Mapped "{[/dagRuns/search],methods=[POST]}" onto public java.util.concurrent.CompletableFuture<za.co.absa.hyperdrive.trigger.models.search.TableSearchResponse<za.co.absa.hyperdrive.trigger.models.dagRuns.DagRun>> za.co.absa.hyperdrive.trigger.api.rest.controllers.DagRunController.searchDagRuns(za.co.absa.hyperdrive.trigger.models.search.TableSearchRequest)
2020-07-13 19:38:23,345 INFO org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping [main] Mapped "{[/jobInstances],methods=[GET]}" onto public java.util.concurrent.CompletableFuture<scala.collection.Seq<za.co.absa.hyperdrive.trigger.models.JobInstance>> za.co.absa.hyperdrive.trigger.api.rest.controllers.JobInstanceController.getJobInstances(long)
2020-07-13 19:38:23,346 INFO org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping [main] Mapped "{[/user/info],methods=[GET]}" onto public za.co.absa.hyperdrive.trigger.models.UserInfo za.co.absa.hyperdrive.trigger.api.rest.controllers.UserInfoController.userInfo()
2020-07-13 19:38:23,347 INFO org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping [main] Mapped "{[/util/quartzDetail],methods=[GET]}" onto public za.co.absa.hyperdrive.trigger.models.QuartzExpressionDetail za.co.absa.hyperdrive.trigger.api.rest.controllers.UtilController.getQuartzDetail(java.lang.String)
2020-07-13 19:38:23,352 INFO org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping [main] Mapped "{[/workflow],methods=[PUT]}" onto public java.util.concurrent.CompletableFuture<za.co.absa.hyperdrive.trigger.models.WorkflowJoined> za.co.absa.hyperdrive.trigger.api.rest.controllers.WorkflowController.createWorkflow(za.co.absa.hyperdrive.trigger.models.WorkflowJoined)
2020-07-13 19:38:23,353 INFO org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping [main] Mapped "{[/workflow],methods=[GET]}" onto public java.util.concurrent.CompletableFuture<za.co.absa.hyperdrive.trigger.models.WorkflowJoined> za.co.absa.hyperdrive.trigger.api.rest.controllers.WorkflowController.getWorkflow(long)
2020-07-13 19:38:23,353 INFO org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping [main] Mapped "{[/workflows],methods=[GET]}" onto public java.util.concurrent.CompletableFuture<scala.collection.Seq<za.co.absa.hyperdrive.trigger.models.Workflow>> za.co.absa.hyperdrive.trigger.api.rest.controllers.WorkflowController.getWorkflows()
2020-07-13 19:38:23,354 INFO org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping [main] Mapped "{[/workflowsByProjectName],methods=[GET]}" onto public java.util.concurrent.CompletableFuture<scala.collection.Seq<za.co.absa.hyperdrive.trigger.models.Workflow>> za.co.absa.hyperdrive.trigger.api.rest.controllers.WorkflowController.getWorkflowsByProjectName(java.lang.String)
2020-07-13 19:38:23,354 INFO org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping [main] Mapped "{[/workflows],methods=[DELETE]}" onto public java.util.concurrent.CompletableFuture<java.lang.Object> za.co.absa.hyperdrive.trigger.api.rest.controllers.WorkflowController.deleteWorkflow(long)
2020-07-13 19:38:23,354 INFO org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping [main] Mapped "{[/workflows],methods=[POST]}" onto public java.util.concurrent.CompletableFuture<za.co.absa.hyperdrive.trigger.models.WorkflowJoined> za.co.absa.hyperdrive.trigger.api.rest.controllers.WorkflowController.updateWorkflow(za.co.absa.hyperdrive.trigger.models.WorkflowJoined)
2020-07-13 19:38:23,355 INFO org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping [main] Mapped "{[/workflows/{id}/switchActiveState],methods=[POST]}" onto public java.util.concurrent.CompletableFuture<java.lang.Object> za.co.absa.hyperdrive.trigger.api.rest.controllers.WorkflowController.switchWorkflowActiveState(long)
2020-07-13 19:38:23,356 INFO org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping [main] Mapped "{[/workflows/projectNames],methods=[GET]}" onto public java.util.concurrent.CompletableFuture<scala.collection.immutable.Set<java.lang.String>> za.co.absa.hyperdrive.trigger.api.rest.controllers.WorkflowController.getProjectNames()
2020-07-13 19:38:23,357 INFO org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping [main] Mapped "{[/workflows/projects],methods=[GET]}" onto public java.util.concurrent.CompletableFuture<scala.collection.Seq<za.co.absa.hyperdrive.trigger.models.Project>> za.co.absa.hyperdrive.trigger.api.rest.controllers.WorkflowController.getProjects()
2020-07-13 19:38:23,357 INFO org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping [main] Mapped "{[/workflows/projectsInfo],methods=[GET]}" onto public java.util.concurrent.CompletableFuture<scala.collection.Seq<za.co.absa.hyperdrive.trigger.models.ProjectInfo>> za.co.absa.hyperdrive.trigger.api.rest.controllers.WorkflowController.getProjectsInfo()
2020-07-13 19:38:23,357 INFO org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping [main] Mapped "{[/workflow/run],methods=[PUT]}" onto public java.util.concurrent.CompletableFuture<java.lang.Object> za.co.absa.hyperdrive.trigger.api.rest.controllers.WorkflowController.runWorkflow(long)
2020-07-13 19:38:23,360 INFO org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping [main] Mapped "{[/error]}" onto public org.springframework.http.ResponseEntity<java.util.Map<java.lang.String, java.lang.Object>> org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController.error(javax.servlet.http.HttpServletRequest)
2020-07-13 19:38:23,362 INFO org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping [main] Mapped "{[/error],produces=[text/html]}" onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
2020-07-13 19:38:23,689 INFO org.springframework.web.servlet.handler.SimpleUrlHandlerMapping [main] Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
2020-07-13 19:38:23,689 INFO org.springframework.web.servlet.handler.SimpleUrlHandlerMapping [main] Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
2020-07-13 19:38:23,716 INFO org.springframework.web.servlet.mvc.method.annotation.ExceptionHandlerExceptionResolver [main] Detected @ExceptionHandler methods in restErrorHandler
2020-07-13 19:38:23,752 INFO org.springframework.web.servlet.handler.SimpleUrlHandlerMapping [main] Mapped URL path [/**/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
2020-07-13 19:38:23,786 INFO org.springframework.boot.autoconfigure.web.servlet.WelcomePageHandlerMapping [main] Adding welcome page: class path resource [ui/index.html]
2020-07-13 19:38:23,845 INFO org.springframework.ldap.core.support.AbstractContextSource [main] Property 'userDn' not set - anonymous context will be used for read-write operations
2020-07-13 19:38:24,118 INFO org.springframework.boot.actuate.endpoint.web.servlet.WebMvcEndpointHandlerMapping [main] Mapped "{[/actuator/health],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
2020-07-13 19:38:24,118 INFO org.springframework.boot.actuate.endpoint.web.servlet.WebMvcEndpointHandlerMapping [main] Mapped "{[/actuator/info],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
2020-07-13 19:38:24,119 INFO org.springframework.boot.actuate.endpoint.web.servlet.WebMvcEndpointHandlerMapping [main] Mapped "{[/actuator],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto protected java.util.Map<java.lang.String, java.util.Map<java.lang.String, org.springframework.boot.actuate.endpoint.web.Link>> org.springframework.boot.actuate.endpoint.web.servlet.WebMvcEndpointHandlerMapping.links(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
2020-07-13 19:38:24,243 INFO org.springframework.jmx.export.annotation.AnnotationMBeanExporter [main] Registering beans for JMX exposure on startup
2020-07-13 19:38:24,271 INFO org.apache.coyote.http11.Http11NioProtocol [main] Starting ProtocolHandler ["http-nio-7123"]
2020-07-13 19:38:24,299 INFO org.apache.tomcat.util.net.NioSelectorPool [main] Using a shared selector for servlet write/read
2020-07-13 19:38:24,316 INFO org.springframework.boot.web.embedded.tomcat.TomcatWebServer [main] Tomcat started on port(s): 7123 (http) with context path ''
2020-07-13 19:38:24,322 INFO scala.App$class [main] Started App.class in 7.932 seconds (JVM running for 9.289)
2020-07-13 19:38:24,396 INFO org.quartz.impl.StdSchedulerFactory [main] Using default implementation for ThreadExecutor
2020-07-13 19:38:24,434 INFO org.quartz.core.SchedulerSignalerImpl [main] Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
2020-07-13 19:38:24,434 INFO org.quartz.core.QuartzScheduler [main] Quartz Scheduler v.2.3.2 created.
2020-07-13 19:38:24,436 INFO org.quartz.simpl.RAMJobStore [main] RAMJobStore initialized.
2020-07-13 19:38:24,438 INFO org.quartz.core.QuartzScheduler [main] Scheduler meta-data: Quartz Scheduler (v2.3.2) 'za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler' with instanceId 'NON_CLUSTERED'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzSchedulerThreadPool' - with 0 threads.
  Using job-store 'org.quartz.simpl.RAMJobStore' - which does not support persistence. and is not clustered.

2020-07-13 19:38:24,438 INFO org.quartz.impl.StdSchedulerFactory [main] Quartz scheduler 'za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler' initialized from an externally provided properties instance.
2020-07-13 19:38:24,439 INFO org.quartz.impl.StdSchedulerFactory [main] Quartz scheduler version: 2.3.2
2020-07-13 19:38:24,439 INFO za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzSchedulerManager$ [main] Starting Quartz Scheduler za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler now
2020-07-13 19:38:24,440 INFO org.quartz.core.QuartzScheduler [main] Scheduler za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler_$_NON_CLUSTERED started.
2020-07-13 19:38:24,447 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-1] Running manager heart beat.
2020-07-13 19:38:24,454 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-4-thread-1] Processing events. Sensors: Set()
2020-07-13 19:38:24,555 INFO com.zaxxer.hikari.HikariDataSource [pool-4-thread-1] db - Starting...
2020-07-13 19:38:24,578 INFO com.zaxxer.hikari.HikariDataSource [pool-4-thread-1] db - Start completed.
2020-07-13 19:38:25,099 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-3] Deploying dag = 646
2020-07-13 19:38:25,139 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-3] Deploying dag = 649
2020-07-13 19:38:25,150 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-3] Deploying dag = 648
2020-07-13 19:38:25,158 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-3] Deploying dag = 655
2020-07-13 19:38:25,166 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-4] Running enqueue finished successfully.
2020-07-13 19:38:25,408 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-3] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = ee3e22b6-44d7-463c-88da-6c0501b47e69
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:25,546 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [http-nio-7123-exec-1] Initializing Spring FrameworkServlet 'dispatcherServlet'
2020-07-13 19:38:25,546 INFO org.springframework.web.servlet.DispatcherServlet [http-nio-7123-exec-1] FrameworkServlet 'dispatcherServlet': initialization started
2020-07-13 19:38:25,569 INFO org.springframework.web.servlet.DispatcherServlet [http-nio-7123-exec-1] FrameworkServlet 'dispatcherServlet': initialization completed in 23 ms
2020-07-13 19:38:26,039 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-1] Running manager heart beat.
2020-07-13 19:38:26,317 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-10] Executing job failed. Job instance id = Some(JobInstance(sene,Spark,JobParameters(Map(jobJar -> sene, mainClass -> sene, deploymentMode -> cluster),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(6b5190ed-3513-4f2f-a511-313d38d110b0),2020-07-03T15:05:05.929,Some(2020-07-03T15:05:06.797),0,649,1297)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:26,317 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-11] Executing job failed. Job instance id = Some(JobInstance(s,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> s, mainClass -> s),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(ce0926e1-7eda-4b14-8ce9-b1847dc6bf37),2020-07-06T13:56:57.320,Some(2020-07-06T17:59:05.011),0,655,1303)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:26,317 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-12] Executing job failed. Job instance id = Some(JobInstance(EDB Demo Token Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/insu/${reader.kafka.topic} , reader.kafka.topic=global.insurance.stdx96.imsv.x96claim.masked, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, transformer.columns.to.select=* , component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader , decoder.avro.value.schema.id=latest, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, decoder.avro.value.schema.naming.strategy=topic.name, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder  , reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.kafka.brokers=http://22.241.27.91:9092, decoder.avro.schema.registry.url=http://22.241.27.91:8081))),Submitting,Some(bcf0c762-bc5d-4dfb-adda-056523177d54),2020-07-03T14:02:11.486,Some(2020-07-03T14:02:12.177),0,648,1296)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:26,317 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-9] Executing job failed. Job instance id = Some(JobInstance(Attunity Accounts Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(reader.option.kafka.sasl.kerberos.service.name=kafka, reader.option.kafka.ssl.key.password=hyperdriveuat, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, reader.kafka.topic=global.flex.kenya.accounts.raw, reader.option.kafka.ssl.keystore.location=hyperdrive-hdp-kafka-uat-keystore.jks, decoder.avro.value.schema.id=latest, decoder.avro.schema.registry.url=https://zadalnrapp1017.corp.dsarena.com:8081, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, reader.option.kafka.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=false useKeyTab=true storeKey=true  useTicketCache=false keyTab="svc-dehdlza-hyperd.keytab" serviceName="kafka" principal="svc-dehdlza-hyperd@CORP.DSARENA.COM";, reader.kafka.brokers=https://zadalnrapp1013.corp.dsarena.com:9093,https://zadalnrapp1014.corp.dsarena.com:9093,https://zadalnrapp1015.corp.dsarena.com:9093, reader.option.kafka.ssl.truststore.location=hyperdrive-hdp-kafka-uat-truststore.jks, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, transformer.columns.to.select=*, component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder, reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.value.schema.naming.strategy=record.name, reader.option.kafka.security.protocol=SASL_PLAINTEXT, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.option.kafka.ssl.truststore.password=hyperdriveuat, writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/attu/${reader.kafka.topic}, decoder.avro.value.schema.record.name=DataRecord, reader.option.kafka.sasl.mechanism=GSSAPI, reader.option.kafka.ssl.keystore.password=hyperdriveuat, decoder.avro.value.schema.record.namespace=com.attunity.queue.msg.Oracle to Kafka with CDC.KEFCRH.CH_ACCT_MAST))),Submitting,Some(79bdeba0-417e-46b1-aa16-1ad3a007c99b),2020-07-03T10:25:28.097,Some(2020-07-03T11:37:58.263),0,646,1293)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:26,341 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:26,934 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:27,044 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-1] Running manager heart beat.
2020-07-13 19:38:27,056 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-6] Deploying dag = 646
2020-07-13 19:38:27,062 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-6] Deploying dag = 649
2020-07-13 19:38:27,068 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-6] Deploying dag = 648
2020-07-13 19:38:27,074 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-16] Executing job failed. Job instance id = Some(JobInstance(Attunity Accounts Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(reader.option.kafka.sasl.kerberos.service.name=kafka, reader.option.kafka.ssl.key.password=hyperdriveuat, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, reader.kafka.topic=global.flex.kenya.accounts.raw, reader.option.kafka.ssl.keystore.location=hyperdrive-hdp-kafka-uat-keystore.jks, decoder.avro.value.schema.id=latest, decoder.avro.schema.registry.url=https://zadalnrapp1017.corp.dsarena.com:8081, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, reader.option.kafka.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=false useKeyTab=true storeKey=true  useTicketCache=false keyTab="svc-dehdlza-hyperd.keytab" serviceName="kafka" principal="svc-dehdlza-hyperd@CORP.DSARENA.COM";, reader.kafka.brokers=https://zadalnrapp1013.corp.dsarena.com:9093,https://zadalnrapp1014.corp.dsarena.com:9093,https://zadalnrapp1015.corp.dsarena.com:9093, reader.option.kafka.ssl.truststore.location=hyperdrive-hdp-kafka-uat-truststore.jks, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, transformer.columns.to.select=*, component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder, reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.value.schema.naming.strategy=record.name, reader.option.kafka.security.protocol=SASL_PLAINTEXT, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.option.kafka.ssl.truststore.password=hyperdriveuat, writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/attu/${reader.kafka.topic}, decoder.avro.value.schema.record.name=DataRecord, reader.option.kafka.sasl.mechanism=GSSAPI, reader.option.kafka.ssl.keystore.password=hyperdriveuat, decoder.avro.value.schema.record.namespace=com.attunity.queue.msg.Oracle to Kafka with CDC.KEFCRH.CH_ACCT_MAST))),Submitting,Some(79bdeba0-417e-46b1-aa16-1ad3a007c99b),2020-07-03T10:25:28.097,Some(2020-07-03T11:37:58.263),0,646,1293)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:27,076 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-6] Deploying dag = 655
2020-07-13 19:38:27,079 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-18] Executing job failed. Job instance id = Some(JobInstance(sene,Spark,JobParameters(Map(jobJar -> sene, mainClass -> sene, deploymentMode -> cluster),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(6b5190ed-3513-4f2f-a511-313d38d110b0),2020-07-03T15:05:05.929,Some(2020-07-03T15:05:06.797),0,649,1297)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:27,083 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-7] Running enqueue finished successfully.
2020-07-13 19:38:27,086 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-22] Executing job failed. Job instance id = Some(JobInstance(EDB Demo Token Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/insu/${reader.kafka.topic} , reader.kafka.topic=global.insurance.stdx96.imsv.x96claim.masked, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, transformer.columns.to.select=* , component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader , decoder.avro.value.schema.id=latest, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, decoder.avro.value.schema.naming.strategy=topic.name, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder  , reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.kafka.brokers=http://22.241.27.91:9092, decoder.avro.schema.registry.url=http://22.241.27.91:8081))),Submitting,Some(bcf0c762-bc5d-4dfb-adda-056523177d54),2020-07-03T14:02:11.486,Some(2020-07-03T14:02:12.177),0,648,1296)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:27,092 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-24] Executing job failed. Job instance id = Some(JobInstance(s,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> s, mainClass -> s),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(ce0926e1-7eda-4b14-8ce9-b1847dc6bf37),2020-07-06T13:56:57.320,Some(2020-07-06T17:59:05.011),0,655,1303)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:28,052 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-1] Running manager heart beat.
2020-07-13 19:38:28,062 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-9] Deploying dag = 646
2020-07-13 19:38:28,066 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-9] Deploying dag = 649
2020-07-13 19:38:28,071 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-9] Deploying dag = 648
2020-07-13 19:38:28,076 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-9] Deploying dag = 655
2020-07-13 19:38:28,080 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-30] Executing job failed. Job instance id = Some(JobInstance(Attunity Accounts Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(reader.option.kafka.sasl.kerberos.service.name=kafka, reader.option.kafka.ssl.key.password=hyperdriveuat, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, reader.kafka.topic=global.flex.kenya.accounts.raw, reader.option.kafka.ssl.keystore.location=hyperdrive-hdp-kafka-uat-keystore.jks, decoder.avro.value.schema.id=latest, decoder.avro.schema.registry.url=https://zadalnrapp1017.corp.dsarena.com:8081, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, reader.option.kafka.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=false useKeyTab=true storeKey=true  useTicketCache=false keyTab="svc-dehdlza-hyperd.keytab" serviceName="kafka" principal="svc-dehdlza-hyperd@CORP.DSARENA.COM";, reader.kafka.brokers=https://zadalnrapp1013.corp.dsarena.com:9093,https://zadalnrapp1014.corp.dsarena.com:9093,https://zadalnrapp1015.corp.dsarena.com:9093, reader.option.kafka.ssl.truststore.location=hyperdrive-hdp-kafka-uat-truststore.jks, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, transformer.columns.to.select=*, component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder, reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.value.schema.naming.strategy=record.name, reader.option.kafka.security.protocol=SASL_PLAINTEXT, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.option.kafka.ssl.truststore.password=hyperdriveuat, writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/attu/${reader.kafka.topic}, decoder.avro.value.schema.record.name=DataRecord, reader.option.kafka.sasl.mechanism=GSSAPI, reader.option.kafka.ssl.keystore.password=hyperdriveuat, decoder.avro.value.schema.record.namespace=com.attunity.queue.msg.Oracle to Kafka with CDC.KEFCRH.CH_ACCT_MAST))),Submitting,Some(79bdeba0-417e-46b1-aa16-1ad3a007c99b),2020-07-03T10:25:28.097,Some(2020-07-03T11:37:58.263),0,646,1293)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:28,080 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-29] Executing job failed. Job instance id = Some(JobInstance(sene,Spark,JobParameters(Map(jobJar -> sene, mainClass -> sene, deploymentMode -> cluster),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(6b5190ed-3513-4f2f-a511-313d38d110b0),2020-07-03T15:05:05.929,Some(2020-07-03T15:05:06.797),0,649,1297)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:28,081 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-10] Running enqueue finished successfully.
2020-07-13 19:38:28,084 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-1] Executing job failed. Job instance id = Some(JobInstance(EDB Demo Token Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/insu/${reader.kafka.topic} , reader.kafka.topic=global.insurance.stdx96.imsv.x96claim.masked, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, transformer.columns.to.select=* , component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader , decoder.avro.value.schema.id=latest, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, decoder.avro.value.schema.naming.strategy=topic.name, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder  , reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.kafka.brokers=http://22.241.27.91:9092, decoder.avro.schema.registry.url=http://22.241.27.91:8081))),Submitting,Some(bcf0c762-bc5d-4dfb-adda-056523177d54),2020-07-03T14:02:11.486,Some(2020-07-03T14:02:12.177),0,648,1296)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:28,088 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-6] Executing job failed. Job instance id = Some(JobInstance(s,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> s, mainClass -> s),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(ce0926e1-7eda-4b14-8ce9-b1847dc6bf37),2020-07-06T13:56:57.320,Some(2020-07-06T17:59:05.011),0,655,1303)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:28,160 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:28,163 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-3] Couldn't create Kafka sensor for sensor (#9).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:28,164 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-3] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 63997a41-4b04-4fec-9597-e6aa311c6edd
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:28,164 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:28,165 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:28,165 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:28,165 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-3] Couldn't create Kafka sensor for sensor (#6).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:28,166 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-3] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 83e0d13f-b6e7-45bf-8d90-a3de03edb7c4
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:28,166 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:28,166 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:28,166 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:28,167 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-3] Couldn't create Kafka sensor for sensor (#7).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:28,167 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-3] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 8872344a-78b0-438b-90af-8318624489d9
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:28,168 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:28,168 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:28,168 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:28,168 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-3] Couldn't create Kafka sensor for sensor (#8).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:28,168 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-3] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 93a9c235-9098-4bf9-8dbc-1ca073c184eb
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:28,169 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:28,169 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:28,169 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:28,170 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-3] Couldn't create Kafka sensor for sensor (#5).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:28,170 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-3] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = cce655fb-c690-422c-96ba-df377f7150d4
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:28,171 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:28,171 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:28,171 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:28,171 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-3] Couldn't create Kafka sensor for sensor (#3).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:28,172 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-3] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 9d802500-c542-4d8f-9850-d451205dda14
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:28,172 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:28,172 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:28,172 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:28,173 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-3] Couldn't create Kafka sensor for sensor (#32).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:28,173 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-3] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = c74736d2-b8af-409f-a060-adf9730bf86b
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:28,174 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:28,174 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:28,174 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:28,175 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-3] Couldn't create Kafka sensor for sensor (#33).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:28,175 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-3] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 6e9fbc7f-84ad-49a8-bca0-5ed933566a0a
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:28,176 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:28,176 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:28,176 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:28,176 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-3] Couldn't create Kafka sensor for sensor (#31).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:28,177 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-3] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 8d208071-d66b-4b7a-b622-ef7072155ab8
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:28,177 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:28,177 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:28,177 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-3] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:28,177 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-3] Couldn't create Kafka sensor for sensor (#42).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:28,182 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-11] Running sensors finished successfully.
2020-07-13 19:38:28,182 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-6] Processing events successful
2020-07-13 19:38:29,058 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-1] Running manager heart beat.
2020-07-13 19:38:29,059 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-4-thread-1] Processing events. Sensors: Set()
2020-07-13 19:38:29,073 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-13] Deploying dag = 646
2020-07-13 19:38:29,075 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-9] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 413dfdf4-c809-4b03-ab38-80af861920d6
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:29,076 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:29,076 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:29,076 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:29,077 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-9] Couldn't create Kafka sensor for sensor (#9).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:29,078 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-9] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 09804416-0afc-4c60-b828-5069cbaf2930
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:29,078 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:29,078 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:29,078 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:29,078 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-13] Deploying dag = 649
2020-07-13 19:38:29,079 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-9] Couldn't create Kafka sensor for sensor (#6).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:29,079 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-9] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 201addde-09bd-4c85-813e-731ee3a2b711
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:29,080 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:29,080 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:29,080 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:29,081 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-9] Couldn't create Kafka sensor for sensor (#7).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:29,082 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-9] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 3b70099f-3dbc-410d-b42b-5b95e551899e
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:29,082 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:29,082 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:29,083 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:29,083 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-9] Couldn't create Kafka sensor for sensor (#8).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:29,084 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-9] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = acbcceb2-adc9-4dde-9fc8-ab61402b1181
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:29,084 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-13] Deploying dag = 648
2020-07-13 19:38:29,085 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:29,085 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:29,085 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:29,085 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-9] Couldn't create Kafka sensor for sensor (#5).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:29,086 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-9] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 283f71c8-d634-4c3c-ab22-43a15a779e3e
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:29,086 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:29,086 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:29,087 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:29,087 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-9] Couldn't create Kafka sensor for sensor (#3).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:29,087 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-10] Executing job failed. Job instance id = Some(JobInstance(Attunity Accounts Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(reader.option.kafka.sasl.kerberos.service.name=kafka, reader.option.kafka.ssl.key.password=hyperdriveuat, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, reader.kafka.topic=global.flex.kenya.accounts.raw, reader.option.kafka.ssl.keystore.location=hyperdrive-hdp-kafka-uat-keystore.jks, decoder.avro.value.schema.id=latest, decoder.avro.schema.registry.url=https://zadalnrapp1017.corp.dsarena.com:8081, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, reader.option.kafka.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=false useKeyTab=true storeKey=true  useTicketCache=false keyTab="svc-dehdlza-hyperd.keytab" serviceName="kafka" principal="svc-dehdlza-hyperd@CORP.DSARENA.COM";, reader.kafka.brokers=https://zadalnrapp1013.corp.dsarena.com:9093,https://zadalnrapp1014.corp.dsarena.com:9093,https://zadalnrapp1015.corp.dsarena.com:9093, reader.option.kafka.ssl.truststore.location=hyperdrive-hdp-kafka-uat-truststore.jks, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, transformer.columns.to.select=*, component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder, reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.value.schema.naming.strategy=record.name, reader.option.kafka.security.protocol=SASL_PLAINTEXT, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.option.kafka.ssl.truststore.password=hyperdriveuat, writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/attu/${reader.kafka.topic}, decoder.avro.value.schema.record.name=DataRecord, reader.option.kafka.sasl.mechanism=GSSAPI, reader.option.kafka.ssl.keystore.password=hyperdriveuat, decoder.avro.value.schema.record.namespace=com.attunity.queue.msg.Oracle to Kafka with CDC.KEFCRH.CH_ACCT_MAST))),Submitting,Some(79bdeba0-417e-46b1-aa16-1ad3a007c99b),2020-07-03T10:25:28.097,Some(2020-07-03T11:37:58.263),0,646,1293)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:29,088 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-9] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = b64a2cc9-f4a5-47a2-8676-098187e41aa8
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:29,088 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:29,088 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:29,088 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:29,089 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-9] Couldn't create Kafka sensor for sensor (#32).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:29,090 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-13] Deploying dag = 655
2020-07-13 19:38:29,090 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-9] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 6efe8cd4-e2ed-4825-9f55-eb3e377c1407
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:29,090 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:29,090 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:29,091 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:29,091 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-12] Executing job failed. Job instance id = Some(JobInstance(sene,Spark,JobParameters(Map(jobJar -> sene, mainClass -> sene, deploymentMode -> cluster),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(6b5190ed-3513-4f2f-a511-313d38d110b0),2020-07-03T15:05:05.929,Some(2020-07-03T15:05:06.797),0,649,1297)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:29,091 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-9] Couldn't create Kafka sensor for sensor (#33).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:29,092 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-9] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 3cb1dad6-57c0-4b15-b6f5-d41c025bb98c
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:29,093 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:29,093 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:29,093 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:29,093 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-9] Couldn't create Kafka sensor for sensor (#31).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:29,094 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-9] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = ae6c2794-1ed3-47e0-b1ed-3ea5cabd0f1d
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:29,095 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:29,095 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:29,095 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-9] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:29,095 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-14] Running enqueue finished successfully.
2020-07-13 19:38:29,095 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-9] Couldn't create Kafka sensor for sensor (#42).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:29,096 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-15] Running sensors finished successfully.
2020-07-13 19:38:29,096 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-12] Processing events successful
2020-07-13 19:38:29,097 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-15] Executing job failed. Job instance id = Some(JobInstance(EDB Demo Token Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/insu/${reader.kafka.topic} , reader.kafka.topic=global.insurance.stdx96.imsv.x96claim.masked, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, transformer.columns.to.select=* , component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader , decoder.avro.value.schema.id=latest, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, decoder.avro.value.schema.naming.strategy=topic.name, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder  , reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.kafka.brokers=http://22.241.27.91:9092, decoder.avro.schema.registry.url=http://22.241.27.91:8081))),Submitting,Some(bcf0c762-bc5d-4dfb-adda-056523177d54),2020-07-03T14:02:11.486,Some(2020-07-03T14:02:12.177),0,648,1296)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:29,102 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-17] Executing job failed. Job instance id = Some(JobInstance(s,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> s, mainClass -> s),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(ce0926e1-7eda-4b14-8ce9-b1847dc6bf37),2020-07-06T13:56:57.320,Some(2020-07-06T17:59:05.011),0,655,1303)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:30,069 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-1] Running manager heart beat.
2020-07-13 19:38:30,075 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-4-thread-1] Processing events. Sensors: Set()
2020-07-13 19:38:30,088 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-17] Deploying dag = 646
2020-07-13 19:38:30,093 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-15] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 2af34497-d435-435a-9e36-cfba825a4547
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:30,094 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:30,094 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-17] Deploying dag = 649
2020-07-13 19:38:30,094 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:30,094 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:30,095 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-15] Couldn't create Kafka sensor for sensor (#9).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:30,096 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-15] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = ea6cf7ee-e8f2-467e-ab2c-75914fd4f4aa
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:30,097 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:30,097 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:30,098 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:30,098 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-15] Couldn't create Kafka sensor for sensor (#6).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:30,099 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-15] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 53b35411-d39a-4c25-aa9b-f597a22292e7
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:30,100 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:30,100 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:30,100 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:30,101 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-15] Couldn't create Kafka sensor for sensor (#7).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:30,101 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-17] Deploying dag = 648
2020-07-13 19:38:30,102 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-15] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = ca2e455e-334f-4828-826a-842421216db5
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:30,102 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:30,102 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:30,102 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:30,103 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-15] Couldn't create Kafka sensor for sensor (#8).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:30,104 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-15] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 7d319103-7310-4274-8c18-264f52275652
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:30,105 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:30,105 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:30,106 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:30,106 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-15] Couldn't create Kafka sensor for sensor (#5).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:30,107 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-15] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = f3bc36f2-3d53-42ef-9ec6-5d85e27eb3b2
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:30,107 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:30,108 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:30,109 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:30,109 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-22] Executing job failed. Job instance id = Some(JobInstance(Attunity Accounts Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(reader.option.kafka.sasl.kerberos.service.name=kafka, reader.option.kafka.ssl.key.password=hyperdriveuat, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, reader.kafka.topic=global.flex.kenya.accounts.raw, reader.option.kafka.ssl.keystore.location=hyperdrive-hdp-kafka-uat-keystore.jks, decoder.avro.value.schema.id=latest, decoder.avro.schema.registry.url=https://zadalnrapp1017.corp.dsarena.com:8081, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, reader.option.kafka.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=false useKeyTab=true storeKey=true  useTicketCache=false keyTab="svc-dehdlza-hyperd.keytab" serviceName="kafka" principal="svc-dehdlza-hyperd@CORP.DSARENA.COM";, reader.kafka.brokers=https://zadalnrapp1013.corp.dsarena.com:9093,https://zadalnrapp1014.corp.dsarena.com:9093,https://zadalnrapp1015.corp.dsarena.com:9093, reader.option.kafka.ssl.truststore.location=hyperdrive-hdp-kafka-uat-truststore.jks, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, transformer.columns.to.select=*, component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder, reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.value.schema.naming.strategy=record.name, reader.option.kafka.security.protocol=SASL_PLAINTEXT, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.option.kafka.ssl.truststore.password=hyperdriveuat, writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/attu/${reader.kafka.topic}, decoder.avro.value.schema.record.name=DataRecord, reader.option.kafka.sasl.mechanism=GSSAPI, reader.option.kafka.ssl.keystore.password=hyperdriveuat, decoder.avro.value.schema.record.namespace=com.attunity.queue.msg.Oracle to Kafka with CDC.KEFCRH.CH_ACCT_MAST))),Submitting,Some(79bdeba0-417e-46b1-aa16-1ad3a007c99b),2020-07-03T10:25:28.097,Some(2020-07-03T11:37:58.263),0,646,1293)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:30,109 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-17] Deploying dag = 655
2020-07-13 19:38:30,109 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-15] Couldn't create Kafka sensor for sensor (#3).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:30,110 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-15] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 408e1680-a565-496f-bcf6-75c76a31d48a
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:30,111 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:30,111 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:30,111 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:30,112 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-15] Couldn't create Kafka sensor for sensor (#32).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:30,114 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-15] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 5a1d28ee-b8b7-4e98-a59e-922f287422dc
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:30,114 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:30,114 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:30,114 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-24] Executing job failed. Job instance id = Some(JobInstance(sene,Spark,JobParameters(Map(jobJar -> sene, mainClass -> sene, deploymentMode -> cluster),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(6b5190ed-3513-4f2f-a511-313d38d110b0),2020-07-03T15:05:05.929,Some(2020-07-03T15:05:06.797),0,649,1297)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:30,115 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:30,115 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-15] Couldn't create Kafka sensor for sensor (#33).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:30,116 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-15] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 7e2d189e-d23d-48d7-970e-20fbb8f2051e
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:30,116 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-18] Running enqueue finished successfully.
2020-07-13 19:38:30,117 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:30,117 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:30,117 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:30,117 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-15] Couldn't create Kafka sensor for sensor (#31).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:30,118 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-26] Executing job failed. Job instance id = Some(JobInstance(EDB Demo Token Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/insu/${reader.kafka.topic} , reader.kafka.topic=global.insurance.stdx96.imsv.x96claim.masked, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, transformer.columns.to.select=* , component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader , decoder.avro.value.schema.id=latest, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, decoder.avro.value.schema.naming.strategy=topic.name, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder  , reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.kafka.brokers=http://22.241.27.91:9092, decoder.avro.schema.registry.url=http://22.241.27.91:8081))),Submitting,Some(bcf0c762-bc5d-4dfb-adda-056523177d54),2020-07-03T14:02:11.486,Some(2020-07-03T14:02:12.177),0,648,1296)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:30,118 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-15] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = f846a0f9-8f06-40ef-91fd-9f54e7924738
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:30,119 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:30,119 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:30,119 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-15] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:30,119 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-15] Couldn't create Kafka sensor for sensor (#42).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:30,120 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-19] Running sensors finished successfully.
2020-07-13 19:38:30,120 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-18] Processing events successful
2020-07-13 19:38:30,124 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-30] Executing job failed. Job instance id = Some(JobInstance(s,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> s, mainClass -> s),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(ce0926e1-7eda-4b14-8ce9-b1847dc6bf37),2020-07-06T13:56:57.320,Some(2020-07-06T17:59:05.011),0,655,1303)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:31,087 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-1] Running manager heart beat.
2020-07-13 19:38:31,087 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-4-thread-1] Processing events. Sensors: Set()
2020-07-13 19:38:31,102 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-20] Deploying dag = 646
2020-07-13 19:38:31,108 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-1] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 2bc5b2a8-339e-4e99-a9ef-df4b1152b2df
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:31,108 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-20] Deploying dag = 649
2020-07-13 19:38:31,108 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:31,108 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:31,109 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:31,109 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-1] Couldn't create Kafka sensor for sensor (#9).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:31,110 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-1] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0d1e83eb-5c0d-4c82-a92b-aeea7dd7c8c3
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:31,110 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:31,111 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:31,111 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:31,111 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-1] Couldn't create Kafka sensor for sensor (#6).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:31,112 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-1] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 72a2c1f6-5a86-4d5e-af01-e9a90e01032b
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:31,113 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:31,113 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-20] Deploying dag = 648
2020-07-13 19:38:31,114 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:31,114 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:31,114 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-1] Couldn't create Kafka sensor for sensor (#7).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:31,115 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-1] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = bce79a38-6068-4f5b-b674-948c8da676ed
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:31,116 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:31,116 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:31,116 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:31,117 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-1] Couldn't create Kafka sensor for sensor (#8).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:31,118 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-1] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 894e0f15-2a1a-48ab-9a32-ffef6fa620d5
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:31,118 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:31,118 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:31,118 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:31,119 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-20] Deploying dag = 655
2020-07-13 19:38:31,119 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-1] Couldn't create Kafka sensor for sensor (#5).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:31,119 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-1] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = e8d83aae-2603-468e-8367-e9069009984d
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:31,120 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:31,121 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:31,121 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:31,121 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-1] Couldn't create Kafka sensor for sensor (#3).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:31,122 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-1] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 67cf52fd-03ff-4281-bf08-714008886f24
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:31,122 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-3] Executing job failed. Job instance id = Some(JobInstance(Attunity Accounts Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(reader.option.kafka.sasl.kerberos.service.name=kafka, reader.option.kafka.ssl.key.password=hyperdriveuat, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, reader.kafka.topic=global.flex.kenya.accounts.raw, reader.option.kafka.ssl.keystore.location=hyperdrive-hdp-kafka-uat-keystore.jks, decoder.avro.value.schema.id=latest, decoder.avro.schema.registry.url=https://zadalnrapp1017.corp.dsarena.com:8081, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, reader.option.kafka.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=false useKeyTab=true storeKey=true  useTicketCache=false keyTab="svc-dehdlza-hyperd.keytab" serviceName="kafka" principal="svc-dehdlza-hyperd@CORP.DSARENA.COM";, reader.kafka.brokers=https://zadalnrapp1013.corp.dsarena.com:9093,https://zadalnrapp1014.corp.dsarena.com:9093,https://zadalnrapp1015.corp.dsarena.com:9093, reader.option.kafka.ssl.truststore.location=hyperdrive-hdp-kafka-uat-truststore.jks, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, transformer.columns.to.select=*, component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder, reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.value.schema.naming.strategy=record.name, reader.option.kafka.security.protocol=SASL_PLAINTEXT, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.option.kafka.ssl.truststore.password=hyperdriveuat, writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/attu/${reader.kafka.topic}, decoder.avro.value.schema.record.name=DataRecord, reader.option.kafka.sasl.mechanism=GSSAPI, reader.option.kafka.ssl.keystore.password=hyperdriveuat, decoder.avro.value.schema.record.namespace=com.attunity.queue.msg.Oracle to Kafka with CDC.KEFCRH.CH_ACCT_MAST))),Submitting,Some(79bdeba0-417e-46b1-aa16-1ad3a007c99b),2020-07-03T10:25:28.097,Some(2020-07-03T11:37:58.263),0,646,1293)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:31,123 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-6] Executing job failed. Job instance id = Some(JobInstance(sene,Spark,JobParameters(Map(jobJar -> sene, mainClass -> sene, deploymentMode -> cluster),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(6b5190ed-3513-4f2f-a511-313d38d110b0),2020-07-03T15:05:05.929,Some(2020-07-03T15:05:06.797),0,649,1297)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:31,123 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:31,123 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:31,123 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:31,123 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-1] Couldn't create Kafka sensor for sensor (#32).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:31,124 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-20] Running enqueue finished successfully.
2020-07-13 19:38:31,124 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-1] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 6cb9e8fc-73d2-4084-99b8-54f4686b2dbf
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:31,125 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:31,125 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:31,125 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:31,125 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-1] Couldn't create Kafka sensor for sensor (#33).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:31,126 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-1] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = f837469f-461a-4317-8c10-35a9b691fb89
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:31,126 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:31,127 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:31,127 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-10] Executing job failed. Job instance id = Some(JobInstance(EDB Demo Token Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/insu/${reader.kafka.topic} , reader.kafka.topic=global.insurance.stdx96.imsv.x96claim.masked, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, transformer.columns.to.select=* , component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader , decoder.avro.value.schema.id=latest, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, decoder.avro.value.schema.naming.strategy=topic.name, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder  , reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.kafka.brokers=http://22.241.27.91:9092, decoder.avro.schema.registry.url=http://22.241.27.91:8081))),Submitting,Some(bcf0c762-bc5d-4dfb-adda-056523177d54),2020-07-03T14:02:11.486,Some(2020-07-03T14:02:12.177),0,648,1296)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:31,127 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:31,127 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-1] Couldn't create Kafka sensor for sensor (#31).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:31,129 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-1] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 80a7eee5-bab7-4a00-83b3-8c319c21eb6b
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:31,130 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:31,130 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:31,130 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:31,131 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-1] Couldn't create Kafka sensor for sensor (#42).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:31,131 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-9] Executing job failed. Job instance id = Some(JobInstance(s,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> s, mainClass -> s),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(ce0926e1-7eda-4b14-8ce9-b1847dc6bf37),2020-07-06T13:56:57.320,Some(2020-07-06T17:59:05.011),0,655,1303)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:31,131 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-1] Processing events successful
2020-07-13 19:38:31,131 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-4] Running sensors finished successfully.
2020-07-13 19:38:32,102 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-1] Running manager heart beat.
2020-07-13 19:38:32,102 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-4-thread-1] Processing events. Sensors: Set()
2020-07-13 19:38:32,116 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-5] Deploying dag = 646
2020-07-13 19:38:32,117 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-7] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 835f59a0-71f5-4577-89ad-41f349338b0b
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:32,118 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:32,119 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:32,119 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:32,119 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-7] Couldn't create Kafka sensor for sensor (#9).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:32,120 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-7] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 4c010989-8d9a-4255-a104-6dc57f92474a
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:32,121 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:32,122 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:32,122 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:32,122 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-5] Deploying dag = 649
2020-07-13 19:38:32,122 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-7] Couldn't create Kafka sensor for sensor (#6).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:32,123 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-7] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 2ff6fede-ef1a-4856-a178-724338af8836
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:32,124 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:32,124 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:32,124 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:32,125 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-7] Couldn't create Kafka sensor for sensor (#7).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:32,125 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-7] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 2c9ece9f-744f-4bab-99bc-ec2a2932b869
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:32,126 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:32,126 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:32,126 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:32,127 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-7] Couldn't create Kafka sensor for sensor (#8).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:32,127 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-5] Deploying dag = 648
2020-07-13 19:38:32,128 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-7] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 8fbacc95-4785-4bda-b7ee-bcee301ff570
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:32,128 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:32,128 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:32,129 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:32,129 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-7] Couldn't create Kafka sensor for sensor (#5).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:32,130 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-7] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 30908811-b715-42d1-a78e-a84d339cf11a
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:32,131 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:32,131 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:32,131 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:32,132 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-7] Couldn't create Kafka sensor for sensor (#3).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:32,132 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-5] Deploying dag = 655
2020-07-13 19:38:32,132 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-7] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 6e6cd140-f17d-4d2f-bc72-32533a01c733
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:32,133 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:32,134 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:32,134 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:32,134 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-15] Executing job failed. Job instance id = Some(JobInstance(Attunity Accounts Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(reader.option.kafka.sasl.kerberos.service.name=kafka, reader.option.kafka.ssl.key.password=hyperdriveuat, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, reader.kafka.topic=global.flex.kenya.accounts.raw, reader.option.kafka.ssl.keystore.location=hyperdrive-hdp-kafka-uat-keystore.jks, decoder.avro.value.schema.id=latest, decoder.avro.schema.registry.url=https://zadalnrapp1017.corp.dsarena.com:8081, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, reader.option.kafka.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=false useKeyTab=true storeKey=true  useTicketCache=false keyTab="svc-dehdlza-hyperd.keytab" serviceName="kafka" principal="svc-dehdlza-hyperd@CORP.DSARENA.COM";, reader.kafka.brokers=https://zadalnrapp1013.corp.dsarena.com:9093,https://zadalnrapp1014.corp.dsarena.com:9093,https://zadalnrapp1015.corp.dsarena.com:9093, reader.option.kafka.ssl.truststore.location=hyperdrive-hdp-kafka-uat-truststore.jks, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, transformer.columns.to.select=*, component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder, reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.value.schema.naming.strategy=record.name, reader.option.kafka.security.protocol=SASL_PLAINTEXT, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.option.kafka.ssl.truststore.password=hyperdriveuat, writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/attu/${reader.kafka.topic}, decoder.avro.value.schema.record.name=DataRecord, reader.option.kafka.sasl.mechanism=GSSAPI, reader.option.kafka.ssl.keystore.password=hyperdriveuat, decoder.avro.value.schema.record.namespace=com.attunity.queue.msg.Oracle to Kafka with CDC.KEFCRH.CH_ACCT_MAST))),Submitting,Some(79bdeba0-417e-46b1-aa16-1ad3a007c99b),2020-07-03T10:25:28.097,Some(2020-07-03T11:37:58.263),0,646,1293)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:32,134 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-7] Couldn't create Kafka sensor for sensor (#32).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:32,135 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-7] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = a62f5edd-e4f8-40b7-8811-dbce95c4c57a
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:32,136 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:32,137 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:32,137 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:32,137 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-18] Executing job failed. Job instance id = Some(JobInstance(sene,Spark,JobParameters(Map(jobJar -> sene, mainClass -> sene, deploymentMode -> cluster),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(6b5190ed-3513-4f2f-a511-313d38d110b0),2020-07-03T15:05:05.929,Some(2020-07-03T15:05:06.797),0,649,1297)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:32,137 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-7] Couldn't create Kafka sensor for sensor (#33).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:32,138 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-7] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 7a3c570f-d22a-4ce5-9942-27f8305df433
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:32,139 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-5] Running enqueue finished successfully.
2020-07-13 19:38:32,139 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:32,140 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:32,140 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:32,140 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-7] Couldn't create Kafka sensor for sensor (#31).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:32,141 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-7] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 70f89706-4aae-4a08-9e3a-68a521136a8a
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:32,143 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:32,143 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:32,143 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:32,144 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-7] Couldn't create Kafka sensor for sensor (#42).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:32,144 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-7] Processing events successful
2020-07-13 19:38:32,144 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-8] Running sensors finished successfully.
2020-07-13 19:38:32,148 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-22] Executing job failed. Job instance id = Some(JobInstance(EDB Demo Token Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/insu/${reader.kafka.topic} , reader.kafka.topic=global.insurance.stdx96.imsv.x96claim.masked, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, transformer.columns.to.select=* , component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader , decoder.avro.value.schema.id=latest, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, decoder.avro.value.schema.naming.strategy=topic.name, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder  , reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.kafka.brokers=http://22.241.27.91:9092, decoder.avro.schema.registry.url=http://22.241.27.91:8081))),Submitting,Some(bcf0c762-bc5d-4dfb-adda-056523177d54),2020-07-03T14:02:11.486,Some(2020-07-03T14:02:12.177),0,648,1296)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:32,148 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-25] Executing job failed. Job instance id = Some(JobInstance(s,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> s, mainClass -> s),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(ce0926e1-7eda-4b14-8ce9-b1847dc6bf37),2020-07-06T13:56:57.320,Some(2020-07-06T17:59:05.011),0,655,1303)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:33,116 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-1] Running manager heart beat.
2020-07-13 19:38:33,116 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-4-thread-1] Processing events. Sensors: Set()
2020-07-13 19:38:33,129 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-9] Deploying dag = 646
2020-07-13 19:38:33,129 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-13] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = c272ce55-47f0-43d1-afc9-4fea0896ca24
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:33,130 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:33,130 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:33,130 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:33,130 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-13] Couldn't create Kafka sensor for sensor (#9).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:33,131 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-13] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 083dcb4b-61ca-41d5-85c0-cebcf462ec8d
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:33,131 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:33,131 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:33,131 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:33,132 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-13] Couldn't create Kafka sensor for sensor (#6).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:33,132 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-9] Deploying dag = 649
2020-07-13 19:38:33,132 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-13] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 2ca4a79e-6a21-42f8-a642-5ffd5ec7dbb5
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:33,133 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:33,133 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:33,133 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:33,133 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-13] Couldn't create Kafka sensor for sensor (#7).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:33,134 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-13] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 6c811816-8e6f-43bf-a3f9-d8d7fd116504
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:33,134 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:33,134 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:33,134 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:33,135 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-13] Couldn't create Kafka sensor for sensor (#8).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:33,135 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-9] Deploying dag = 648
2020-07-13 19:38:33,136 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-13] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 8cc263c7-9198-42d6-984f-3b9e9db60bcf
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:33,136 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:33,136 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:33,136 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:33,137 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-13] Couldn't create Kafka sensor for sensor (#5).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:33,139 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-9] Deploying dag = 655
2020-07-13 19:38:33,140 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-13] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = bb21b5b1-2454-4607-9efb-dea41a4d5ddc
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:33,141 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:33,141 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:33,141 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:33,141 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-13] Couldn't create Kafka sensor for sensor (#3).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:33,142 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-13] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = c556a7e2-56bc-4821-bda7-83f45874dfcf
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:33,143 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-9] Running enqueue finished successfully.
2020-07-13 19:38:33,143 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:33,143 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:33,143 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:33,144 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-13] Couldn't create Kafka sensor for sensor (#32).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:33,145 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-26] Executing job failed. Job instance id = Some(JobInstance(Attunity Accounts Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(reader.option.kafka.sasl.kerberos.service.name=kafka, reader.option.kafka.ssl.key.password=hyperdriveuat, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, reader.kafka.topic=global.flex.kenya.accounts.raw, reader.option.kafka.ssl.keystore.location=hyperdrive-hdp-kafka-uat-keystore.jks, decoder.avro.value.schema.id=latest, decoder.avro.schema.registry.url=https://zadalnrapp1017.corp.dsarena.com:8081, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, reader.option.kafka.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=false useKeyTab=true storeKey=true  useTicketCache=false keyTab="svc-dehdlza-hyperd.keytab" serviceName="kafka" principal="svc-dehdlza-hyperd@CORP.DSARENA.COM";, reader.kafka.brokers=https://zadalnrapp1013.corp.dsarena.com:9093,https://zadalnrapp1014.corp.dsarena.com:9093,https://zadalnrapp1015.corp.dsarena.com:9093, reader.option.kafka.ssl.truststore.location=hyperdrive-hdp-kafka-uat-truststore.jks, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, transformer.columns.to.select=*, component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder, reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.value.schema.naming.strategy=record.name, reader.option.kafka.security.protocol=SASL_PLAINTEXT, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.option.kafka.ssl.truststore.password=hyperdriveuat, writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/attu/${reader.kafka.topic}, decoder.avro.value.schema.record.name=DataRecord, reader.option.kafka.sasl.mechanism=GSSAPI, reader.option.kafka.ssl.keystore.password=hyperdriveuat, decoder.avro.value.schema.record.namespace=com.attunity.queue.msg.Oracle to Kafka with CDC.KEFCRH.CH_ACCT_MAST))),Submitting,Some(79bdeba0-417e-46b1-aa16-1ad3a007c99b),2020-07-03T10:25:28.097,Some(2020-07-03T11:37:58.263),0,646,1293)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:33,145 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-13] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 52fa5c32-6b0b-4b1a-bde4-7e4ada4674ba
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:33,145 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-26] Executing job failed. Job instance id = Some(JobInstance(sene,Spark,JobParameters(Map(jobJar -> sene, mainClass -> sene, deploymentMode -> cluster),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(6b5190ed-3513-4f2f-a511-313d38d110b0),2020-07-03T15:05:05.929,Some(2020-07-03T15:05:06.797),0,649,1297)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:33,145 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:33,146 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:33,146 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:33,146 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-13] Couldn't create Kafka sensor for sensor (#33).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:33,146 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-5] Executing job failed. Job instance id = Some(JobInstance(EDB Demo Token Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/insu/${reader.kafka.topic} , reader.kafka.topic=global.insurance.stdx96.imsv.x96claim.masked, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, transformer.columns.to.select=* , component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader , decoder.avro.value.schema.id=latest, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, decoder.avro.value.schema.naming.strategy=topic.name, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder  , reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.kafka.brokers=http://22.241.27.91:9092, decoder.avro.schema.registry.url=http://22.241.27.91:8081))),Submitting,Some(bcf0c762-bc5d-4dfb-adda-056523177d54),2020-07-03T14:02:11.486,Some(2020-07-03T14:02:12.177),0,648,1296)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:33,147 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-13] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 457c6417-725d-48fe-b138-4a557d8b0cee
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:33,147 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:33,147 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:33,147 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:33,147 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-13] Couldn't create Kafka sensor for sensor (#31).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:33,148 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-13] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dc6c03b8-a0e3-418e-99b0-40b3ef776126
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:33,149 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:33,149 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-3] Executing job failed. Job instance id = Some(JobInstance(s,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> s, mainClass -> s),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(ce0926e1-7eda-4b14-8ce9-b1847dc6bf37),2020-07-06T13:56:57.320,Some(2020-07-06T17:59:05.011),0,655,1303)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:33,149 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:33,149 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:33,149 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-13] Couldn't create Kafka sensor for sensor (#42).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:33,149 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-13] Processing events successful
2020-07-13 19:38:33,149 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-12] Running sensors finished successfully.
2020-07-13 19:38:34,125 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-1] Running manager heart beat.
2020-07-13 19:38:34,126 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-4-thread-1] Processing events. Sensors: Set()
2020-07-13 19:38:34,139 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-13] Deploying dag = 646
2020-07-13 19:38:34,141 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-19] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = ab69b918-8603-4016-bf6e-e3293e3d8bbf
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:34,142 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:34,142 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:34,142 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:34,142 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-19] Couldn't create Kafka sensor for sensor (#9).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:34,143 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-19] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 233409f7-bbe0-459a-8575-021be128703b
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:34,143 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-13] Deploying dag = 649
2020-07-13 19:38:34,144 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:34,144 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:34,144 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:34,144 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-19] Couldn't create Kafka sensor for sensor (#6).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:34,145 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-19] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = ab37012b-c136-43c3-8c11-b283a011722a
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:34,145 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:34,145 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:34,145 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:34,146 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-19] Couldn't create Kafka sensor for sensor (#7).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:34,146 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-13] Deploying dag = 648
2020-07-13 19:38:34,147 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-19] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = bb7b31c4-13f7-4987-8f13-42bf95718f3b
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:34,147 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:34,147 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:34,147 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:34,147 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-19] Couldn't create Kafka sensor for sensor (#8).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:34,148 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-19] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 2ae6246b-c470-460b-9dd4-5c98f3e73dbd
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:34,150 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:34,150 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:34,150 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:34,151 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-19] Couldn't create Kafka sensor for sensor (#5).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:34,151 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-13] Deploying dag = 655
2020-07-13 19:38:34,152 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-19] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 9c0c9646-5513-485a-8151-3c9cc4f2e6bb
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:34,152 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:34,152 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:34,152 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:34,153 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-10] Executing job failed. Job instance id = Some(JobInstance(Attunity Accounts Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(reader.option.kafka.sasl.kerberos.service.name=kafka, reader.option.kafka.ssl.key.password=hyperdriveuat, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, reader.kafka.topic=global.flex.kenya.accounts.raw, reader.option.kafka.ssl.keystore.location=hyperdrive-hdp-kafka-uat-keystore.jks, decoder.avro.value.schema.id=latest, decoder.avro.schema.registry.url=https://zadalnrapp1017.corp.dsarena.com:8081, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, reader.option.kafka.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=false useKeyTab=true storeKey=true  useTicketCache=false keyTab="svc-dehdlza-hyperd.keytab" serviceName="kafka" principal="svc-dehdlza-hyperd@CORP.DSARENA.COM";, reader.kafka.brokers=https://zadalnrapp1013.corp.dsarena.com:9093,https://zadalnrapp1014.corp.dsarena.com:9093,https://zadalnrapp1015.corp.dsarena.com:9093, reader.option.kafka.ssl.truststore.location=hyperdrive-hdp-kafka-uat-truststore.jks, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, transformer.columns.to.select=*, component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder, reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.value.schema.naming.strategy=record.name, reader.option.kafka.security.protocol=SASL_PLAINTEXT, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.option.kafka.ssl.truststore.password=hyperdriveuat, writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/attu/${reader.kafka.topic}, decoder.avro.value.schema.record.name=DataRecord, reader.option.kafka.sasl.mechanism=GSSAPI, reader.option.kafka.ssl.keystore.password=hyperdriveuat, decoder.avro.value.schema.record.namespace=com.attunity.queue.msg.Oracle to Kafka with CDC.KEFCRH.CH_ACCT_MAST))),Submitting,Some(79bdeba0-417e-46b1-aa16-1ad3a007c99b),2020-07-03T10:25:28.097,Some(2020-07-03T11:37:58.263),0,646,1293)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:34,153 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-19] Couldn't create Kafka sensor for sensor (#3).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:34,154 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-19] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 64e7b8d0-5b58-4fe6-b2c7-08488c5a335e
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:34,154 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-12] Executing job failed. Job instance id = Some(JobInstance(sene,Spark,JobParameters(Map(jobJar -> sene, mainClass -> sene, deploymentMode -> cluster),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(6b5190ed-3513-4f2f-a511-313d38d110b0),2020-07-03T15:05:05.929,Some(2020-07-03T15:05:06.797),0,649,1297)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:34,154 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:34,155 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:34,155 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-13] Running enqueue finished successfully.
2020-07-13 19:38:34,155 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:34,156 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-19] Couldn't create Kafka sensor for sensor (#32).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:34,156 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-19] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = aaee276d-a34c-4e11-a289-86a6fb3128cc
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:34,157 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:34,158 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:34,158 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:34,158 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-14] Executing job failed. Job instance id = Some(JobInstance(EDB Demo Token Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/insu/${reader.kafka.topic} , reader.kafka.topic=global.insurance.stdx96.imsv.x96claim.masked, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, transformer.columns.to.select=* , component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader , decoder.avro.value.schema.id=latest, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, decoder.avro.value.schema.naming.strategy=topic.name, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder  , reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.kafka.brokers=http://22.241.27.91:9092, decoder.avro.schema.registry.url=http://22.241.27.91:8081))),Submitting,Some(bcf0c762-bc5d-4dfb-adda-056523177d54),2020-07-03T14:02:11.486,Some(2020-07-03T14:02:12.177),0,648,1296)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:34,158 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-19] Couldn't create Kafka sensor for sensor (#33).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:34,159 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-19] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = debaf644-22c8-4494-a95b-1a92f3f19416
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:34,159 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:34,159 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:34,159 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:34,160 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-19] Couldn't create Kafka sensor for sensor (#31).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:34,160 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-19] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 27c9fd25-cf28-47d7-8829-d143d9f86f69
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:34,161 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:34,161 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-17] Executing job failed. Job instance id = Some(JobInstance(s,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> s, mainClass -> s),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(ce0926e1-7eda-4b14-8ce9-b1847dc6bf37),2020-07-06T13:56:57.320,Some(2020-07-06T17:59:05.011),0,655,1303)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:34,161 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:34,161 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:34,161 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-19] Couldn't create Kafka sensor for sensor (#42).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:34,161 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-19] Processing events successful
2020-07-13 19:38:34,161 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-16] Running sensors finished successfully.
2020-07-13 19:38:35,138 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-1] Running manager heart beat.
2020-07-13 19:38:35,138 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-4-thread-1] Processing events. Sensors: Set()
2020-07-13 19:38:35,150 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-17] Deploying dag = 646
2020-07-13 19:38:35,152 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-6] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 547bfa66-9084-4d81-ba68-751ed3f1ca07
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:35,152 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:35,152 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:35,152 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:35,153 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-6] Couldn't create Kafka sensor for sensor (#9).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:35,153 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-6] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = a6911315-d1e3-41ad-b7ad-b616aa7cbd77
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:35,153 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:35,154 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:35,154 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:35,154 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-17] Deploying dag = 649
2020-07-13 19:38:35,154 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-6] Couldn't create Kafka sensor for sensor (#6).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:35,154 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-6] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = b50efd23-1e2d-4b59-859f-dd619c80e264
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:35,155 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:35,155 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:35,155 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:35,155 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-6] Couldn't create Kafka sensor for sensor (#7).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:35,156 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-6] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = b0903f85-3869-4ffe-be09-e33ff3b4c5d7
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:35,156 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:35,157 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:35,157 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:35,157 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-6] Couldn't create Kafka sensor for sensor (#8).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:35,158 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-17] Deploying dag = 648
2020-07-13 19:38:35,159 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-6] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 306068bf-2f50-4a47-b00a-ec9f63b61f03
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:35,159 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:35,159 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:35,159 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:35,160 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-6] Couldn't create Kafka sensor for sensor (#5).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:35,160 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-6] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 4366675e-9833-4d20-a0cc-74d2c4d5ba79
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:35,161 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:35,161 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-17] Deploying dag = 655
2020-07-13 19:38:35,161 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:35,161 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-21] Executing job failed. Job instance id = Some(JobInstance(Attunity Accounts Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(reader.option.kafka.sasl.kerberos.service.name=kafka, reader.option.kafka.ssl.key.password=hyperdriveuat, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, reader.kafka.topic=global.flex.kenya.accounts.raw, reader.option.kafka.ssl.keystore.location=hyperdrive-hdp-kafka-uat-keystore.jks, decoder.avro.value.schema.id=latest, decoder.avro.schema.registry.url=https://zadalnrapp1017.corp.dsarena.com:8081, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, reader.option.kafka.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=false useKeyTab=true storeKey=true  useTicketCache=false keyTab="svc-dehdlza-hyperd.keytab" serviceName="kafka" principal="svc-dehdlza-hyperd@CORP.DSARENA.COM";, reader.kafka.brokers=https://zadalnrapp1013.corp.dsarena.com:9093,https://zadalnrapp1014.corp.dsarena.com:9093,https://zadalnrapp1015.corp.dsarena.com:9093, reader.option.kafka.ssl.truststore.location=hyperdrive-hdp-kafka-uat-truststore.jks, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, transformer.columns.to.select=*, component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder, reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.value.schema.naming.strategy=record.name, reader.option.kafka.security.protocol=SASL_PLAINTEXT, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.option.kafka.ssl.truststore.password=hyperdriveuat, writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/attu/${reader.kafka.topic}, decoder.avro.value.schema.record.name=DataRecord, reader.option.kafka.sasl.mechanism=GSSAPI, reader.option.kafka.ssl.keystore.password=hyperdriveuat, decoder.avro.value.schema.record.namespace=com.attunity.queue.msg.Oracle to Kafka with CDC.KEFCRH.CH_ACCT_MAST))),Submitting,Some(79bdeba0-417e-46b1-aa16-1ad3a007c99b),2020-07-03T10:25:28.097,Some(2020-07-03T11:37:58.263),0,646,1293)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:35,161 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:35,162 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-6] Couldn't create Kafka sensor for sensor (#3).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:35,162 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-6] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 998769a3-5a35-4ab9-9cf8-460566037f7d
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:35,163 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:35,163 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:35,163 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:35,163 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-6] Couldn't create Kafka sensor for sensor (#32).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:35,164 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-6] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 77340a4b-84ec-4916-af85-f39e24bb44bd
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:35,164 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-22] Executing job failed. Job instance id = Some(JobInstance(sene,Spark,JobParameters(Map(jobJar -> sene, mainClass -> sene, deploymentMode -> cluster),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(6b5190ed-3513-4f2f-a511-313d38d110b0),2020-07-03T15:05:05.929,Some(2020-07-03T15:05:06.797),0,649,1297)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:35,165 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-17] Running enqueue finished successfully.
2020-07-13 19:38:35,166 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:35,166 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:35,166 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:35,167 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-6] Couldn't create Kafka sensor for sensor (#33).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:35,167 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-6] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 5def15b3-c4de-44c3-8047-dbca01094000
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:35,168 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:35,168 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:35,168 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:35,169 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-6] Couldn't create Kafka sensor for sensor (#31).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:35,170 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-6] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 6d0a2c26-243d-4404-82a8-e3336f443828
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:35,170 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-4] Executing job failed. Job instance id = Some(JobInstance(EDB Demo Token Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/insu/${reader.kafka.topic} , reader.kafka.topic=global.insurance.stdx96.imsv.x96claim.masked, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, transformer.columns.to.select=* , component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader , decoder.avro.value.schema.id=latest, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, decoder.avro.value.schema.naming.strategy=topic.name, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder  , reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.kafka.brokers=http://22.241.27.91:9092, decoder.avro.schema.registry.url=http://22.241.27.91:8081))),Submitting,Some(bcf0c762-bc5d-4dfb-adda-056523177d54),2020-07-03T14:02:11.486,Some(2020-07-03T14:02:12.177),0,648,1296)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:35,170 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:35,170 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:35,170 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-6] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:35,171 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-6] Couldn't create Kafka sensor for sensor (#42).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:35,171 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-6] Processing events successful
2020-07-13 19:38:35,171 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-2] Running sensors finished successfully.
2020-07-13 19:38:35,174 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-27] Executing job failed. Job instance id = Some(JobInstance(s,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> s, mainClass -> s),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(ce0926e1-7eda-4b14-8ce9-b1847dc6bf37),2020-07-06T13:56:57.320,Some(2020-07-06T17:59:05.011),0,655,1303)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:36,149 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-1] Running manager heart beat.
2020-07-13 19:38:36,149 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-4-thread-1] Processing events. Sensors: Set()
2020-07-13 19:38:36,167 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-3] Deploying dag = 646
2020-07-13 19:38:36,168 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-12] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = cf48f40e-0372-4c10-a354-2a2eb9081625
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:36,169 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:36,169 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:36,169 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:36,170 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-12] Couldn't create Kafka sensor for sensor (#9).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:36,171 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-12] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = f58d39dc-fcd6-4f45-9ea4-05a1445b2279
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:36,171 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:36,172 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:36,172 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:36,172 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-3] Deploying dag = 649
2020-07-13 19:38:36,172 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-12] Couldn't create Kafka sensor for sensor (#6).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:36,174 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-12] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 9a61b948-6f08-4131-945b-86b40faca2e7
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:36,175 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:36,175 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:36,177 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-3] Deploying dag = 648
2020-07-13 19:38:36,177 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:36,178 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-12] Couldn't create Kafka sensor for sensor (#7).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:36,179 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-12] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = f4e908f0-6e71-4128-83c2-363ec601d96d
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:36,179 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:36,179 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:36,180 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:36,180 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-3] Deploying dag = 655
2020-07-13 19:38:36,180 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-12] Couldn't create Kafka sensor for sensor (#8).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:36,181 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-12] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = ae8ae7a6-989e-45c0-a58b-125f48b2e12a
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:36,183 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:36,183 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:36,183 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:36,184 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-12] Couldn't create Kafka sensor for sensor (#5).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:36,184 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-12] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = d795587a-2a98-4f85-93f2-01c878beabc2
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:36,184 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-3] Running enqueue finished successfully.
2020-07-13 19:38:36,185 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:36,185 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:36,185 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:36,186 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-12] Couldn't create Kafka sensor for sensor (#3).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:36,186 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-12] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 71de2f59-6ed3-4a88-b2f3-aaa5ad7f05ed
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:36,187 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:36,187 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:36,187 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:36,188 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-12] Couldn't create Kafka sensor for sensor (#32).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:36,189 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-12] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = cfe82700-bc02-4cf8-be13-408c8c571220
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:36,190 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:36,190 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:36,190 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:36,191 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-12] Couldn't create Kafka sensor for sensor (#33).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:36,192 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-6] Executing job failed. Job instance id = Some(JobInstance(Attunity Accounts Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(reader.option.kafka.sasl.kerberos.service.name=kafka, reader.option.kafka.ssl.key.password=hyperdriveuat, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, reader.kafka.topic=global.flex.kenya.accounts.raw, reader.option.kafka.ssl.keystore.location=hyperdrive-hdp-kafka-uat-keystore.jks, decoder.avro.value.schema.id=latest, decoder.avro.schema.registry.url=https://zadalnrapp1017.corp.dsarena.com:8081, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, reader.option.kafka.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=false useKeyTab=true storeKey=true  useTicketCache=false keyTab="svc-dehdlza-hyperd.keytab" serviceName="kafka" principal="svc-dehdlza-hyperd@CORP.DSARENA.COM";, reader.kafka.brokers=https://zadalnrapp1013.corp.dsarena.com:9093,https://zadalnrapp1014.corp.dsarena.com:9093,https://zadalnrapp1015.corp.dsarena.com:9093, reader.option.kafka.ssl.truststore.location=hyperdrive-hdp-kafka-uat-truststore.jks, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, transformer.columns.to.select=*, component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder, reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.value.schema.naming.strategy=record.name, reader.option.kafka.security.protocol=SASL_PLAINTEXT, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.option.kafka.ssl.truststore.password=hyperdriveuat, writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/attu/${reader.kafka.topic}, decoder.avro.value.schema.record.name=DataRecord, reader.option.kafka.sasl.mechanism=GSSAPI, reader.option.kafka.ssl.keystore.password=hyperdriveuat, decoder.avro.value.schema.record.namespace=com.attunity.queue.msg.Oracle to Kafka with CDC.KEFCRH.CH_ACCT_MAST))),Submitting,Some(79bdeba0-417e-46b1-aa16-1ad3a007c99b),2020-07-03T10:25:28.097,Some(2020-07-03T11:37:58.263),0,646,1293)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:36,192 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-12] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = bc92b5ab-8864-4061-a97e-d1cbadc0fae8
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:36,192 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-6] Executing job failed. Job instance id = Some(JobInstance(sene,Spark,JobParameters(Map(jobJar -> sene, mainClass -> sene, deploymentMode -> cluster),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(6b5190ed-3513-4f2f-a511-313d38d110b0),2020-07-03T15:05:05.929,Some(2020-07-03T15:05:06.797),0,649,1297)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:36,192 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:36,192 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:36,193 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:36,193 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-6] Executing job failed. Job instance id = Some(JobInstance(EDB Demo Token Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/insu/${reader.kafka.topic} , reader.kafka.topic=global.insurance.stdx96.imsv.x96claim.masked, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, transformer.columns.to.select=* , component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader , decoder.avro.value.schema.id=latest, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, decoder.avro.value.schema.naming.strategy=topic.name, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder  , reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.kafka.brokers=http://22.241.27.91:9092, decoder.avro.schema.registry.url=http://22.241.27.91:8081))),Submitting,Some(bcf0c762-bc5d-4dfb-adda-056523177d54),2020-07-03T14:02:11.486,Some(2020-07-03T14:02:12.177),0,648,1296)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:36,193 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-13] Executing job failed. Job instance id = Some(JobInstance(s,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> s, mainClass -> s),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(ce0926e1-7eda-4b14-8ce9-b1847dc6bf37),2020-07-06T13:56:57.320,Some(2020-07-06T17:59:05.011),0,655,1303)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:36,193 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-12] Couldn't create Kafka sensor for sensor (#31).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:36,193 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-12] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 74ad355e-be72-4674-b10e-bbb986449426
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:36,194 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:36,194 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:36,194 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-12] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:36,194 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-12] Couldn't create Kafka sensor for sensor (#42).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:36,195 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-6] Running sensors finished successfully.
2020-07-13 19:38:36,195 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-12] Processing events successful
2020-07-13 19:38:37,165 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-1] Running manager heart beat.
2020-07-13 19:38:37,166 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-4-thread-1] Processing events. Sensors: Set()
2020-07-13 19:38:37,178 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-5] Deploying dag = 646
2020-07-13 19:38:37,179 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-18] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 5cd7bad2-e7c0-4a49-8b0b-9820a5082888
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:37,180 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-5] Deploying dag = 649
2020-07-13 19:38:37,181 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:37,182 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:37,182 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:37,183 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-18] Couldn't create Kafka sensor for sensor (#9).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:37,184 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-18] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 1057c22e-110d-4de5-9c17-ca0d41f35ef4
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:37,184 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-5] Deploying dag = 648
2020-07-13 19:38:37,184 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:37,184 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:37,184 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:37,185 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-18] Couldn't create Kafka sensor for sensor (#6).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:37,186 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-18] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 4d136ec1-353e-4809-ac0f-ce02a4d682a1
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:37,186 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:37,187 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:37,187 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:37,187 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-5] Deploying dag = 655
2020-07-13 19:38:37,187 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-18] Couldn't create Kafka sensor for sensor (#7).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:37,188 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-18] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 9a45e57e-a833-4d83-8488-b9c0c8583548
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:37,189 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:37,189 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:37,189 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:37,189 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-18] Couldn't create Kafka sensor for sensor (#8).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:37,190 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-18] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 54afd866-d2b9-4b81-9f70-f231762b818a
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:37,190 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-5] Running enqueue finished successfully.
2020-07-13 19:38:37,191 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:37,191 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:37,191 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:37,191 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-18] Couldn't create Kafka sensor for sensor (#5).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:37,192 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-18] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 541350a0-fdce-4622-8824-c0294c7d8abe
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:37,193 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:37,193 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:37,193 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:37,194 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-18] Couldn't create Kafka sensor for sensor (#3).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:37,194 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-14] Executing job failed. Job instance id = Some(JobInstance(Attunity Accounts Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(reader.option.kafka.sasl.kerberos.service.name=kafka, reader.option.kafka.ssl.key.password=hyperdriveuat, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, reader.kafka.topic=global.flex.kenya.accounts.raw, reader.option.kafka.ssl.keystore.location=hyperdrive-hdp-kafka-uat-keystore.jks, decoder.avro.value.schema.id=latest, decoder.avro.schema.registry.url=https://zadalnrapp1017.corp.dsarena.com:8081, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, reader.option.kafka.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=false useKeyTab=true storeKey=true  useTicketCache=false keyTab="svc-dehdlza-hyperd.keytab" serviceName="kafka" principal="svc-dehdlza-hyperd@CORP.DSARENA.COM";, reader.kafka.brokers=https://zadalnrapp1013.corp.dsarena.com:9093,https://zadalnrapp1014.corp.dsarena.com:9093,https://zadalnrapp1015.corp.dsarena.com:9093, reader.option.kafka.ssl.truststore.location=hyperdrive-hdp-kafka-uat-truststore.jks, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, transformer.columns.to.select=*, component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder, reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.value.schema.naming.strategy=record.name, reader.option.kafka.security.protocol=SASL_PLAINTEXT, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.option.kafka.ssl.truststore.password=hyperdriveuat, writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/attu/${reader.kafka.topic}, decoder.avro.value.schema.record.name=DataRecord, reader.option.kafka.sasl.mechanism=GSSAPI, reader.option.kafka.ssl.keystore.password=hyperdriveuat, decoder.avro.value.schema.record.namespace=com.attunity.queue.msg.Oracle to Kafka with CDC.KEFCRH.CH_ACCT_MAST))),Submitting,Some(79bdeba0-417e-46b1-aa16-1ad3a007c99b),2020-07-03T10:25:28.097,Some(2020-07-03T11:37:58.263),0,646,1293)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:37,194 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-18] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 9c6c7494-48a2-412f-bdbc-2a9dec3e96dd
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:37,195 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:37,195 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:37,195 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:37,196 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-18] Couldn't create Kafka sensor for sensor (#32).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:37,197 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-18] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 8e854ddc-07e2-40fc-8724-ca249857df01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:37,197 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:37,198 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:37,198 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:37,198 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-18] Couldn't create Kafka sensor for sensor (#33).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:37,199 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-18] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = c8d1deb6-06f8-4087-a79b-b7900d665a26
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:37,199 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-19] Executing job failed. Job instance id = Some(JobInstance(sene,Spark,JobParameters(Map(jobJar -> sene, mainClass -> sene, deploymentMode -> cluster),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(6b5190ed-3513-4f2f-a511-313d38d110b0),2020-07-03T15:05:05.929,Some(2020-07-03T15:05:06.797),0,649,1297)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:37,201 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:37,201 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-24] Executing job failed. Job instance id = Some(JobInstance(EDB Demo Token Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/insu/${reader.kafka.topic} , reader.kafka.topic=global.insurance.stdx96.imsv.x96claim.masked, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, transformer.columns.to.select=* , component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader , decoder.avro.value.schema.id=latest, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, decoder.avro.value.schema.naming.strategy=topic.name, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder  , reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.kafka.brokers=http://22.241.27.91:9092, decoder.avro.schema.registry.url=http://22.241.27.91:8081))),Submitting,Some(bcf0c762-bc5d-4dfb-adda-056523177d54),2020-07-03T14:02:11.486,Some(2020-07-03T14:02:12.177),0,648,1296)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:37,201 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:37,201 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:37,201 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-22] Executing job failed. Job instance id = Some(JobInstance(s,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> s, mainClass -> s),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(ce0926e1-7eda-4b14-8ce9-b1847dc6bf37),2020-07-06T13:56:57.320,Some(2020-07-06T17:59:05.011),0,655,1303)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:37,201 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-18] Couldn't create Kafka sensor for sensor (#31).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:37,202 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-18] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = ea3cf3cc-eab7-444e-8643-076db5415c23
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:37,203 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:37,203 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:37,203 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-18] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:37,203 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-18] Couldn't create Kafka sensor for sensor (#42).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:37,204 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-10] Running sensors finished successfully.
2020-07-13 19:38:37,204 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-18] Processing events successful
2020-07-13 19:38:38,175 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-1] Running manager heart beat.
2020-07-13 19:38:38,175 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-4-thread-1] Processing events. Sensors: Set()
2020-07-13 19:38:38,186 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-1] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 149a8d5f-92f4-42cc-93c6-02e3df948e76
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:38,186 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:38,186 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:38,187 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-11] Deploying dag = 646
2020-07-13 19:38:38,187 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:38,188 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-1] Couldn't create Kafka sensor for sensor (#9).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:38,188 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-1] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dceff42a-fa9f-4f2e-9ee9-10ab5b593793
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:38,189 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:38,189 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:38,189 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:38,189 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-1] Couldn't create Kafka sensor for sensor (#6).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:38,190 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-1] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 93c73a4a-2a9e-428b-aff7-993c48e9770d
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:38,190 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:38,190 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:38,190 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:38,190 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-11] Deploying dag = 649
2020-07-13 19:38:38,191 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-1] Couldn't create Kafka sensor for sensor (#7).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:38,191 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-1] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = b7f54be5-6f14-4427-8ce3-b4811083a2f3
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:38,192 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:38,192 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:38,192 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:38,192 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-1] Couldn't create Kafka sensor for sensor (#8).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:38,193 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-1] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 256e2380-59d1-4713-b2d0-a2023df9bec5
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:38,194 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:38,194 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-11] Deploying dag = 648
2020-07-13 19:38:38,194 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:38,194 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:38,194 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-1] Couldn't create Kafka sensor for sensor (#5).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:38,195 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-1] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 10dc7125-21a2-48a4-8e86-187b3e369075
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:38,195 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:38,196 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:38,196 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:38,196 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-1] Couldn't create Kafka sensor for sensor (#3).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:38,196 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-11] Deploying dag = 655
2020-07-13 19:38:38,197 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-1] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 04e15b4d-7378-4b27-a6b7-fa36e44475d1
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:38,197 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:38,197 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:38,198 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:38,198 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-1] Couldn't create Kafka sensor for sensor (#32).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:38,199 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-1] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 904c6d10-2a32-4ff6-aa2e-82f07b5bf9de
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:38,199 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:38,199 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:38,199 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-11] Running enqueue finished successfully.
2020-07-13 19:38:38,199 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:38,200 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-1] Couldn't create Kafka sensor for sensor (#33).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:38,201 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-1] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = d5d8f5f6-8295-4f35-838a-1df4c9f72e01
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:38,201 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:38,201 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:38,201 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:38,202 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-1] Couldn't create Kafka sensor for sensor (#31).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:38,203 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-1] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 1504321d-6811-4f42-b352-a517cc24fa3d
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:38,203 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-2] Executing job failed. Job instance id = Some(JobInstance(Attunity Accounts Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(reader.option.kafka.sasl.kerberos.service.name=kafka, reader.option.kafka.ssl.key.password=hyperdriveuat, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, reader.kafka.topic=global.flex.kenya.accounts.raw, reader.option.kafka.ssl.keystore.location=hyperdrive-hdp-kafka-uat-keystore.jks, decoder.avro.value.schema.id=latest, decoder.avro.schema.registry.url=https://zadalnrapp1017.corp.dsarena.com:8081, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, reader.option.kafka.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=false useKeyTab=true storeKey=true  useTicketCache=false keyTab="svc-dehdlza-hyperd.keytab" serviceName="kafka" principal="svc-dehdlza-hyperd@CORP.DSARENA.COM";, reader.kafka.brokers=https://zadalnrapp1013.corp.dsarena.com:9093,https://zadalnrapp1014.corp.dsarena.com:9093,https://zadalnrapp1015.corp.dsarena.com:9093, reader.option.kafka.ssl.truststore.location=hyperdrive-hdp-kafka-uat-truststore.jks, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, transformer.columns.to.select=*, component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder, reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.value.schema.naming.strategy=record.name, reader.option.kafka.security.protocol=SASL_PLAINTEXT, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.option.kafka.ssl.truststore.password=hyperdriveuat, writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/attu/${reader.kafka.topic}, decoder.avro.value.schema.record.name=DataRecord, reader.option.kafka.sasl.mechanism=GSSAPI, reader.option.kafka.ssl.keystore.password=hyperdriveuat, decoder.avro.value.schema.record.namespace=com.attunity.queue.msg.Oracle to Kafka with CDC.KEFCRH.CH_ACCT_MAST))),Submitting,Some(79bdeba0-417e-46b1-aa16-1ad3a007c99b),2020-07-03T10:25:28.097,Some(2020-07-03T11:37:58.263),0,646,1293)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:38,203 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:38,203 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:38,203 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-1] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:38,203 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-26] Executing job failed. Job instance id = Some(JobInstance(sene,Spark,JobParameters(Map(jobJar -> sene, mainClass -> sene, deploymentMode -> cluster),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(6b5190ed-3513-4f2f-a511-313d38d110b0),2020-07-03T15:05:05.929,Some(2020-07-03T15:05:06.797),0,649,1297)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:38,204 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-1] Couldn't create Kafka sensor for sensor (#42).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:38,204 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-29] Executing job failed. Job instance id = Some(JobInstance(EDB Demo Token Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/insu/${reader.kafka.topic} , reader.kafka.topic=global.insurance.stdx96.imsv.x96claim.masked, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, transformer.columns.to.select=* , component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader , decoder.avro.value.schema.id=latest, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, decoder.avro.value.schema.naming.strategy=topic.name, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder  , reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.kafka.brokers=http://22.241.27.91:9092, decoder.avro.schema.registry.url=http://22.241.27.91:8081))),Submitting,Some(bcf0c762-bc5d-4dfb-adda-056523177d54),2020-07-03T14:02:11.486,Some(2020-07-03T14:02:12.177),0,648,1296)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:38,204 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-1] Processing events successful
2020-07-13 19:38:38,204 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-14] Running sensors finished successfully.
2020-07-13 19:38:38,205 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-5] Executing job failed. Job instance id = Some(JobInstance(s,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> s, mainClass -> s),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(ce0926e1-7eda-4b14-8ce9-b1847dc6bf37),2020-07-06T13:56:57.320,Some(2020-07-06T17:59:05.011),0,655,1303)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:39,183 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-1] Running manager heart beat.
2020-07-13 19:38:39,183 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-4-thread-1] Processing events. Sensors: Set()
2020-07-13 19:38:39,195 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-15] Deploying dag = 646
2020-07-13 19:38:39,198 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-15] Deploying dag = 649
2020-07-13 19:38:39,199 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-7] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 900815b3-8fad-455e-8a4c-abb129401f00
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:39,199 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:39,199 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:39,199 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:39,200 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-7] Couldn't create Kafka sensor for sensor (#9).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:39,201 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-7] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = d19d842e-bad9-404d-abe7-0199094cb90f
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:39,202 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-15] Deploying dag = 648
2020-07-13 19:38:39,202 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:39,202 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:39,202 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:39,203 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-7] Couldn't create Kafka sensor for sensor (#6).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:39,204 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-7] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = be07e84c-3a6f-4ec6-9709-fa0395faa719
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:39,204 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:39,204 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:39,204 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:39,205 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-7] Couldn't create Kafka sensor for sensor (#7).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:39,205 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-15] Deploying dag = 655
2020-07-13 19:38:39,206 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-7] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dbc72138-b08e-40a5-81d7-1e2e1e08d882
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:39,206 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:39,206 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:39,206 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:39,207 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-7] Couldn't create Kafka sensor for sensor (#8).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:39,207 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-7] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = c7b870bd-2a8e-4de1-83d3-c905f4949eac
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:39,208 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:39,208 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:39,208 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:39,209 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-15] Running enqueue finished successfully.
2020-07-13 19:38:39,209 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-7] Couldn't create Kafka sensor for sensor (#5).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:39,210 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-7] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 62375a16-7b9e-49de-8d4a-68c1fe89e897
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:39,211 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:39,211 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:39,211 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:39,211 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-7] Couldn't create Kafka sensor for sensor (#3).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:39,212 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-7] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 8211bce9-f31a-4c70-83f5-a752e4d3497e
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:39,212 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:39,213 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:39,213 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:39,213 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-7] Couldn't create Kafka sensor for sensor (#32).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:39,213 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-6] Executing job failed. Job instance id = Some(JobInstance(Attunity Accounts Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(reader.option.kafka.sasl.kerberos.service.name=kafka, reader.option.kafka.ssl.key.password=hyperdriveuat, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, reader.kafka.topic=global.flex.kenya.accounts.raw, reader.option.kafka.ssl.keystore.location=hyperdrive-hdp-kafka-uat-keystore.jks, decoder.avro.value.schema.id=latest, decoder.avro.schema.registry.url=https://zadalnrapp1017.corp.dsarena.com:8081, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, reader.option.kafka.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=false useKeyTab=true storeKey=true  useTicketCache=false keyTab="svc-dehdlza-hyperd.keytab" serviceName="kafka" principal="svc-dehdlza-hyperd@CORP.DSARENA.COM";, reader.kafka.brokers=https://zadalnrapp1013.corp.dsarena.com:9093,https://zadalnrapp1014.corp.dsarena.com:9093,https://zadalnrapp1015.corp.dsarena.com:9093, reader.option.kafka.ssl.truststore.location=hyperdrive-hdp-kafka-uat-truststore.jks, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, transformer.columns.to.select=*, component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder, reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.value.schema.naming.strategy=record.name, reader.option.kafka.security.protocol=SASL_PLAINTEXT, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.option.kafka.ssl.truststore.password=hyperdriveuat, writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/attu/${reader.kafka.topic}, decoder.avro.value.schema.record.name=DataRecord, reader.option.kafka.sasl.mechanism=GSSAPI, reader.option.kafka.ssl.keystore.password=hyperdriveuat, decoder.avro.value.schema.record.namespace=com.attunity.queue.msg.Oracle to Kafka with CDC.KEFCRH.CH_ACCT_MAST))),Submitting,Some(79bdeba0-417e-46b1-aa16-1ad3a007c99b),2020-07-03T10:25:28.097,Some(2020-07-03T11:37:58.263),0,646,1293)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:39,214 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-7] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 5531566d-75cb-43c0-9653-a0943a663b5f
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:39,214 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:39,214 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:39,214 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:39,214 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-9] Executing job failed. Job instance id = Some(JobInstance(sene,Spark,JobParameters(Map(jobJar -> sene, mainClass -> sene, deploymentMode -> cluster),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(6b5190ed-3513-4f2f-a511-313d38d110b0),2020-07-03T15:05:05.929,Some(2020-07-03T15:05:06.797),0,649,1297)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:39,215 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-7] Couldn't create Kafka sensor for sensor (#33).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:39,215 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-14] Executing job failed. Job instance id = Some(JobInstance(EDB Demo Token Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/insu/${reader.kafka.topic} , reader.kafka.topic=global.insurance.stdx96.imsv.x96claim.masked, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, transformer.columns.to.select=* , component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader , decoder.avro.value.schema.id=latest, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, decoder.avro.value.schema.naming.strategy=topic.name, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder  , reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.kafka.brokers=http://22.241.27.91:9092, decoder.avro.schema.registry.url=http://22.241.27.91:8081))),Submitting,Some(bcf0c762-bc5d-4dfb-adda-056523177d54),2020-07-03T14:02:11.486,Some(2020-07-03T14:02:12.177),0,648,1296)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:39,215 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-15] Executing job failed. Job instance id = Some(JobInstance(s,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> s, mainClass -> s),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(ce0926e1-7eda-4b14-8ce9-b1847dc6bf37),2020-07-06T13:56:57.320,Some(2020-07-06T17:59:05.011),0,655,1303)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:39,215 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-7] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 96dd8fba-d24c-433b-beb4-f97ec2961cb4
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:39,216 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:39,216 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:39,216 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:39,217 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-7] Couldn't create Kafka sensor for sensor (#31).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:39,218 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-7] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 22782526-670a-427f-82c1-a9e9713dc1eb
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:39,219 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:39,219 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:39,219 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-7] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:39,219 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-7] Couldn't create Kafka sensor for sensor (#42).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:39,219 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-7] Processing events successful
2020-07-13 19:38:39,219 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-18] Running sensors finished successfully.
2020-07-13 19:38:40,195 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-1] Running manager heart beat.
2020-07-13 19:38:40,196 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-4-thread-1] Processing events. Sensors: Set()
2020-07-13 19:38:40,206 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-17] Deploying dag = 646
2020-07-13 19:38:40,207 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-13] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = eb4b91ec-97c6-4436-86ec-271ae160c86f
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:40,208 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:40,208 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:40,208 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:40,208 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-13] Couldn't create Kafka sensor for sensor (#9).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:40,209 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-13] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 9e7d98c2-a6f1-472b-875a-73f8777f68f8
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:40,209 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:40,209 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-17] Deploying dag = 649
2020-07-13 19:38:40,209 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:40,209 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:40,210 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-13] Couldn't create Kafka sensor for sensor (#6).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:40,210 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-13] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 27e0b7d6-ba02-4018-bc40-f7aace9b0b12
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:40,211 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:40,211 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:40,211 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:40,211 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-13] Couldn't create Kafka sensor for sensor (#7).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:40,212 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-17] Deploying dag = 648
2020-07-13 19:38:40,212 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-13] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = bde4a9e9-f7fc-4f03-aec4-3f61ecae2ad1
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:40,213 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:40,213 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:40,213 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:40,213 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-13] Couldn't create Kafka sensor for sensor (#8).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:40,214 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-13] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 5c95028c-fc97-4915-afd0-61e31dac6e40
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:40,215 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:40,215 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:40,215 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:40,215 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-17] Deploying dag = 655
2020-07-13 19:38:40,215 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-13] Couldn't create Kafka sensor for sensor (#5).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:40,216 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-13] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 240d7ffa-04a2-4e77-a945-602074e8dabe
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:40,216 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:40,217 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:40,217 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:40,217 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-13] Couldn't create Kafka sensor for sensor (#3).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:40,217 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-17] Running enqueue finished successfully.
2020-07-13 19:38:40,218 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-13] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = bef33174-88ec-4cd1-b7f9-4f7c541d8616
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:40,218 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:40,218 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:40,219 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:40,219 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-13] Couldn't create Kafka sensor for sensor (#32).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:40,220 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-13] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 06c075e4-9cc7-4502-903c-88ee03cee505
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:40,220 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:40,220 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:40,221 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:40,221 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-13] Couldn't create Kafka sensor for sensor (#33).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:40,222 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-13] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 98cd8412-1da5-4439-b6ae-bf5f79fd561a
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:40,222 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:40,222 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:40,222 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:40,222 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-23] Executing job failed. Job instance id = Some(JobInstance(Attunity Accounts Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(reader.option.kafka.sasl.kerberos.service.name=kafka, reader.option.kafka.ssl.key.password=hyperdriveuat, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, reader.kafka.topic=global.flex.kenya.accounts.raw, reader.option.kafka.ssl.keystore.location=hyperdrive-hdp-kafka-uat-keystore.jks, decoder.avro.value.schema.id=latest, decoder.avro.schema.registry.url=https://zadalnrapp1017.corp.dsarena.com:8081, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, reader.option.kafka.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=false useKeyTab=true storeKey=true  useTicketCache=false keyTab="svc-dehdlza-hyperd.keytab" serviceName="kafka" principal="svc-dehdlza-hyperd@CORP.DSARENA.COM";, reader.kafka.brokers=https://zadalnrapp1013.corp.dsarena.com:9093,https://zadalnrapp1014.corp.dsarena.com:9093,https://zadalnrapp1015.corp.dsarena.com:9093, reader.option.kafka.ssl.truststore.location=hyperdrive-hdp-kafka-uat-truststore.jks, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, transformer.columns.to.select=*, component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder, reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.value.schema.naming.strategy=record.name, reader.option.kafka.security.protocol=SASL_PLAINTEXT, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.option.kafka.ssl.truststore.password=hyperdriveuat, writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/attu/${reader.kafka.topic}, decoder.avro.value.schema.record.name=DataRecord, reader.option.kafka.sasl.mechanism=GSSAPI, reader.option.kafka.ssl.keystore.password=hyperdriveuat, decoder.avro.value.schema.record.namespace=com.attunity.queue.msg.Oracle to Kafka with CDC.KEFCRH.CH_ACCT_MAST))),Submitting,Some(79bdeba0-417e-46b1-aa16-1ad3a007c99b),2020-07-03T10:25:28.097,Some(2020-07-03T11:37:58.263),0,646,1293)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:40,223 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-13] Couldn't create Kafka sensor for sensor (#31).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:40,223 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-30] Executing job failed. Job instance id = Some(JobInstance(s,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> s, mainClass -> s),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(ce0926e1-7eda-4b14-8ce9-b1847dc6bf37),2020-07-06T13:56:57.320,Some(2020-07-06T17:59:05.011),0,655,1303)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:40,223 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-25] Executing job failed. Job instance id = Some(JobInstance(sene,Spark,JobParameters(Map(jobJar -> sene, mainClass -> sene, deploymentMode -> cluster),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(6b5190ed-3513-4f2f-a511-313d38d110b0),2020-07-03T15:05:05.929,Some(2020-07-03T15:05:06.797),0,649,1297)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:40,223 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-4] Executing job failed. Job instance id = Some(JobInstance(EDB Demo Token Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/insu/${reader.kafka.topic} , reader.kafka.topic=global.insurance.stdx96.imsv.x96claim.masked, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, transformer.columns.to.select=* , component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader , decoder.avro.value.schema.id=latest, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, decoder.avro.value.schema.naming.strategy=topic.name, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder  , reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.kafka.brokers=http://22.241.27.91:9092, decoder.avro.schema.registry.url=http://22.241.27.91:8081))),Submitting,Some(bcf0c762-bc5d-4dfb-adda-056523177d54),2020-07-03T14:02:11.486,Some(2020-07-03T14:02:12.177),0,648,1296)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:40,223 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-13] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 9f1b5b44-ca29-4b1f-9b11-5d729b356b85
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:40,223 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:40,223 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:40,224 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-13] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:40,224 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-13] Couldn't create Kafka sensor for sensor (#42).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:40,224 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-17] Processing events successful
2020-07-13 19:38:40,224 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-20] Running sensors finished successfully.
2020-07-13 19:38:41,207 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-1] Running manager heart beat.
2020-07-13 19:38:41,208 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-4-thread-1] Processing events. Sensors: Set()
2020-07-13 19:38:41,217 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-4] Deploying dag = 646
2020-07-13 19:38:41,220 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-19] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 90a264d3-1dab-4334-a4ed-a3ed798463b1
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:41,220 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:41,220 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-4] Deploying dag = 649
2020-07-13 19:38:41,220 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:41,220 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:41,221 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-19] Couldn't create Kafka sensor for sensor (#9).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:41,221 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-19] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = b116fbc9-8ed1-4ee7-b206-a50e0715b8e2
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:41,222 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:41,222 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:41,222 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:41,222 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-19] Couldn't create Kafka sensor for sensor (#6).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:41,223 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-19] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = d24ee9f8-b165-4736-9a70-ec99fb26cad8
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:41,223 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-4] Deploying dag = 648
2020-07-13 19:38:41,223 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:41,223 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:41,223 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:41,224 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-19] Couldn't create Kafka sensor for sensor (#7).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:41,225 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-19] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 53a2e9d0-aacb-4c58-853d-9f0257a0f366
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:41,227 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:41,227 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:41,227 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-4] Deploying dag = 655
2020-07-13 19:38:41,227 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:41,228 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-19] Couldn't create Kafka sensor for sensor (#8).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:41,228 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-19] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = ad634dc2-7b86-4b2b-b1ff-0eb61e6bde44
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:41,229 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:41,229 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:41,229 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:41,229 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-19] Couldn't create Kafka sensor for sensor (#5).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:41,229 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-4] Running enqueue finished successfully.
2020-07-13 19:38:41,230 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-19] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = fd36683c-4416-4f7c-9a6b-a3648f680594
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:41,230 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:41,230 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:41,231 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:41,231 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-19] Couldn't create Kafka sensor for sensor (#3).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:41,231 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-19] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = ce5a9b60-5a48-482f-a592-80ef0a2566db
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:41,232 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:41,232 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:41,232 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:41,232 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-19] Couldn't create Kafka sensor for sensor (#32).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:41,233 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-19] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 71067c15-1f1c-4406-a9bb-90d8748ffb77
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:41,234 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:41,235 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:41,235 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:41,235 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-1] Executing job failed. Job instance id = Some(JobInstance(Attunity Accounts Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(reader.option.kafka.sasl.kerberos.service.name=kafka, reader.option.kafka.ssl.key.password=hyperdriveuat, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, reader.kafka.topic=global.flex.kenya.accounts.raw, reader.option.kafka.ssl.keystore.location=hyperdrive-hdp-kafka-uat-keystore.jks, decoder.avro.value.schema.id=latest, decoder.avro.schema.registry.url=https://zadalnrapp1017.corp.dsarena.com:8081, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, reader.option.kafka.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=false useKeyTab=true storeKey=true  useTicketCache=false keyTab="svc-dehdlza-hyperd.keytab" serviceName="kafka" principal="svc-dehdlza-hyperd@CORP.DSARENA.COM";, reader.kafka.brokers=https://zadalnrapp1013.corp.dsarena.com:9093,https://zadalnrapp1014.corp.dsarena.com:9093,https://zadalnrapp1015.corp.dsarena.com:9093, reader.option.kafka.ssl.truststore.location=hyperdrive-hdp-kafka-uat-truststore.jks, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, transformer.columns.to.select=*, component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder, reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.value.schema.naming.strategy=record.name, reader.option.kafka.security.protocol=SASL_PLAINTEXT, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.option.kafka.ssl.truststore.password=hyperdriveuat, writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/attu/${reader.kafka.topic}, decoder.avro.value.schema.record.name=DataRecord, reader.option.kafka.sasl.mechanism=GSSAPI, reader.option.kafka.ssl.keystore.password=hyperdriveuat, decoder.avro.value.schema.record.namespace=com.attunity.queue.msg.Oracle to Kafka with CDC.KEFCRH.CH_ACCT_MAST))),Submitting,Some(79bdeba0-417e-46b1-aa16-1ad3a007c99b),2020-07-03T10:25:28.097,Some(2020-07-03T11:37:58.263),0,646,1293)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=79bdeba0-417e-46b1-aa16-1ad3a007c99b could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:41,235 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-7] Executing job failed. Job instance id = Some(JobInstance(sene,Spark,JobParameters(Map(jobJar -> sene, mainClass -> sene, deploymentMode -> cluster),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(6b5190ed-3513-4f2f-a511-313d38d110b0),2020-07-03T15:05:05.929,Some(2020-07-03T15:05:06.797),0,649,1297)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=6b5190ed-3513-4f2f-a511-313d38d110b0 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:41,235 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-11] Executing job failed. Job instance id = Some(JobInstance(EDB Demo Token Ingestion,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> /home/svc-dehdlza-hyperd/driver.jar, mainClass -> za.co.absa.hyperdrive.driver.drivers.CommandLineIngestionDriver),Map(appArguments -> List(writer.parquet.destination.directory=/bigdatahdfs/datalake/raw/HyperDrive/buffer/insu/${reader.kafka.topic} , reader.kafka.topic=global.insurance.stdx96.imsv.x96claim.masked, component.transformer=za.co.absa.hyperdrive.ingestor.implementation.transformer.column.selection.ColumnSelectorStreamTransformer, transformer.columns.to.select=* , component.reader=za.co.absa.hyperdrive.ingestor.implementation.reader.kafka.KafkaStreamReader , decoder.avro.value.schema.id=latest, ingestor.spark.app.name=ingestor-app-pane, component.ingestor=Spark, component.manager=za.co.absa.hyperdrive.ingestor.implementation.manager.checkpoint.CheckpointOffsetManager, decoder.avro.value.schema.naming.strategy=topic.name, manager.checkpoint.base.location=/bigdatahdfs/datalake/raw/HyperDrive/checkpoint-location/dev/${reader.kafka.topic}, component.decoder=za.co.absa.hyperdrive.ingestor.implementation.decoder.avro.confluent.ConfluentAvroKafkaStreamDecoder  , reader.option.failOnDataLoss=false, component.writer=za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter, decoder.avro.schema.retention.policy=RETAIN_SELECTED_COLUMN_ONLY, reader.kafka.brokers=http://22.241.27.91:9092, decoder.avro.schema.registry.url=http://22.241.27.91:8081))),Submitting,Some(bcf0c762-bc5d-4dfb-adda-056523177d54),2020-07-03T14:02:11.486,Some(2020-07-03T14:02:12.177),0,648,1296)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=bcf0c762-bc5d-4dfb-adda-056523177d54 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:41,235 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-19] Couldn't create Kafka sensor for sensor (#33).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:41,236 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-19] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 2f687516-9c25-4c01-9d3f-db5f1a7e4e5b
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:41,236 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.Executors [pool-3-thread-10] Executing job failed. Job instance id = Some(JobInstance(s,Spark,JobParameters(Map(deploymentMode -> cluster, jobJar -> s, mainClass -> s),Map(additionalJars -> List(), additionalFiles -> List(), appArguments -> List())),Submitting,Some(ce0926e1-7eda-4b14-8ce9-b1847dc6bf37),2020-07-06T13:56:57.320,Some(2020-07-06T17:59:05.011),0,655,1303)).
java.lang.IllegalArgumentException: Invalid URL /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:85)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.url(StandaloneAhcWSClient.scala:43)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.updateJobStatus(SparkExecutor.scala:61)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.spark.SparkExecutor$.execute(SparkExecutor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:55)
	at za.co.absa.hyperdrive.trigger.scheduler.executors.Executors$$anonfun$executeDag$1$$anonfun$3.apply(Executors.scala:52)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253)
	at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: /ws/v1/cluster/apps?applicationTags=ce0926e1-7eda-4b14-8ce9-b1847dc6bf37 could not be parsed into a proper Uri, missing scheme
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:70)
	at play.shaded.ahc.org.asynchttpclient.uri.Uri.create(Uri.java:62)
	at play.api.libs.ws.ahc.StandaloneAhcWSClient.validate(StandaloneAhcWSClient.scala:82)
	... 11 common frames omitted
2020-07-13 19:38:41,236 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:41,236 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:41,236 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:41,237 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-19] Couldn't create Kafka sensor for sensor (#31).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:41,237 INFO org.apache.kafka.clients.consumer.ConsumerConfig [pool-2-thread-19] ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [https://zadalnrapp1013.corp.dsarena.com:9093, https://zadalnrapp1014.corp.dsarena.com:9093, https://zadalnrapp1015.corp.dsarena.com:9093]
	check.crcs = true
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 515fa3b9-7fcb-40ac-afbe-bbe6d6870227
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 3
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2020-07-13 19:38:41,237 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1013.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1013.corp.dsarena.com
2020-07-13 19:38:41,237 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1014.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1014.corp.dsarena.com
2020-07-13 19:38:41,237 WARN org.apache.kafka.clients.ClientUtils [pool-2-thread-19] Couldn't resolve server https://zadalnrapp1015.corp.dsarena.com:9093 from bootstrap.servers as DNS resolution failed for zadalnrapp1015.corp.dsarena.com
2020-07-13 19:38:41,238 ERROR za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-19] Couldn't create Kafka sensor for sensor (#42).
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:811)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:659)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:639)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.kafka.KafkaSensor.<init>(KafkaSensor.scala:44)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5$$anonfun$2.apply(Sensors.scala:91)
	at scala.util.Try$.apply(Try.scala:192)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:91)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1$$anonfun$apply$5.apply(Sensors.scala:88)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors$$anonfun$za$co$absa$hyperdrive$trigger$scheduler$sensors$Sensors$$addNewSensors$1.apply(Sensors.scala:88)
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Success.map(Try.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:90)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:49)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:721)
	... 23 common frames omitted
2020-07-13 19:38:41,238 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-2-thread-19] Processing events successful
2020-07-13 19:38:41,238 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-7] Running sensors finished successfully.
2020-07-13 19:38:42,022 INFO za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzSchedulerManager$ [http-nio-7123-exec-2] Stopping Quartz Scheduler za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler now
2020-07-13 19:38:42,022 INFO org.quartz.core.QuartzScheduler [http-nio-7123-exec-2] Scheduler za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler_$_NON_CLUSTERED paused.
2020-07-13 19:38:42,023 INFO za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzSchedulerManager$ [http-nio-7123-exec-2] Stopped Quartz Scheduler za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler
2020-07-13 19:38:42,217 DEBUG za.co.absa.hyperdrive.trigger.scheduler.JobScheduler [pool-4-thread-1] Manager stopped.
2020-07-13 20:35:29,081 DEBUG za.co.absa.hyperdrive.trigger.scheduler.eventProcessor.EventProcessor [ScalaTest-main-running-EventProcessorTest] Processing events. Sensor id: 1. Events: List(0)
2020-07-13 20:35:29,099 DEBUG za.co.absa.hyperdrive.trigger.scheduler.eventProcessor.EventProcessor [ScalaTest-main-running-EventProcessorTest] Processing events. Sensor id: 1. Events: List(0)
2020-07-13 20:35:29,105 DEBUG za.co.absa.hyperdrive.trigger.scheduler.eventProcessor.EventProcessor [ScalaTest-main-running-EventProcessorTest] Processing events. Sensor id: 1. Events: List(0)
2020-07-13 20:35:29,343 INFO org.quartz.impl.StdSchedulerFactory [ScalaTest-main-running-TimeSensorIntegrationTest] Using default implementation for ThreadExecutor
2020-07-13 20:35:29,359 INFO org.quartz.core.SchedulerSignalerImpl [ScalaTest-main-running-TimeSensorIntegrationTest] Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
2020-07-13 20:35:29,360 INFO org.quartz.core.QuartzScheduler [ScalaTest-main-running-TimeSensorIntegrationTest] Quartz Scheduler v.2.3.2 created.
2020-07-13 20:35:29,363 INFO org.quartz.simpl.RAMJobStore [ScalaTest-main-running-TimeSensorIntegrationTest] RAMJobStore initialized.
2020-07-13 20:35:29,365 INFO org.quartz.core.QuartzScheduler [ScalaTest-main-running-TimeSensorIntegrationTest] Scheduler meta-data: Quartz Scheduler (v2.3.2) 'za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler' with instanceId 'NON_CLUSTERED'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzSchedulerThreadPool' - with 0 threads.
  Using job-store 'org.quartz.simpl.RAMJobStore' - which does not support persistence. and is not clustered.

2020-07-13 20:35:29,365 INFO org.quartz.impl.StdSchedulerFactory [ScalaTest-main-running-TimeSensorIntegrationTest] Quartz scheduler 'za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler' initialized from an externally provided properties instance.
2020-07-13 20:35:29,365 INFO org.quartz.impl.StdSchedulerFactory [ScalaTest-main-running-TimeSensorIntegrationTest] Quartz scheduler version: 2.3.2
2020-07-13 20:35:29,366 INFO za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzSchedulerManager$ [ScalaTest-main-running-TimeSensorIntegrationTest] Starting Quartz Scheduler za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler now
2020-07-13 20:35:29,366 INFO org.quartz.core.QuartzScheduler [ScalaTest-main-running-TimeSensorIntegrationTest] Scheduler za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler_$_NON_CLUSTERED started.
2020-07-13 20:35:29,367 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [ScalaTest-main-running-TimeSensorIntegrationTest] Processing events. Sensors: Set()
2020-07-13 20:35:29,408 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-1-thread-6] Processing events successful
2020-07-13 20:35:30,017 INFO za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzJob [TimeSensorQuartz-thread-1] Starting Time Sensor Quartz Job
2020-07-13 20:35:30,026 DEBUG za.co.absa.hyperdrive.trigger.scheduler.eventProcessor.EventProcessor [TimeSensorQuartz-thread-1] Processing events. Sensor id: 1. Events: List(0)
2020-07-13 20:35:33,006 INFO za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzJob [TimeSensorQuartz-thread-2] Starting Time Sensor Quartz Job
2020-07-13 20:35:33,009 DEBUG za.co.absa.hyperdrive.trigger.scheduler.eventProcessor.EventProcessor [TimeSensorQuartz-thread-2] Processing events. Sensor id: 1. Events: List(0)
2020-07-13 20:35:34,417 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [ScalaTest-main-running-TimeSensorIntegrationTest] Processing events. Sensors: Set(1)
2020-07-13 20:35:34,423 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-1-thread-7] Processing events successful
2020-07-13 20:35:34,438 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [ScalaTest-main-running-TimeSensorIntegrationTest] Processing events. Sensors: Set(1)
2020-07-13 20:35:34,447 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-1-thread-13] Processing events successful
2020-07-13 20:35:34,449 INFO za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzSchedulerManager$ [ScalaTest-main-running-TimeSensorIntegrationTest] Stopping Quartz Scheduler za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler now
2020-07-13 20:35:34,450 INFO org.quartz.core.QuartzScheduler [ScalaTest-main-running-TimeSensorIntegrationTest] Scheduler za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler_$_NON_CLUSTERED paused.
2020-07-13 20:35:34,450 INFO za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzSchedulerManager$ [ScalaTest-main-running-TimeSensorIntegrationTest] Stopped Quartz Scheduler za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler
2020-07-13 20:35:34,648 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.shell.ShellExecutor$ [Thread-5] echo
2020-07-13 20:44:28,137 DEBUG za.co.absa.hyperdrive.trigger.scheduler.eventProcessor.EventProcessor [ScalaTest-main-running-EventProcessorTest] Processing events. Sensor id: 1. Events: List(0)
2020-07-13 20:44:28,158 DEBUG za.co.absa.hyperdrive.trigger.scheduler.eventProcessor.EventProcessor [ScalaTest-main-running-EventProcessorTest] Processing events. Sensor id: 1. Events: List(0)
2020-07-13 20:44:28,165 DEBUG za.co.absa.hyperdrive.trigger.scheduler.eventProcessor.EventProcessor [ScalaTest-main-running-EventProcessorTest] Processing events. Sensor id: 1. Events: List(0)
2020-07-13 20:44:28,297 INFO org.quartz.impl.StdSchedulerFactory [ScalaTest-main-running-TimeSensorIntegrationTest] Using default implementation for ThreadExecutor
2020-07-13 20:44:28,317 INFO org.quartz.core.SchedulerSignalerImpl [ScalaTest-main-running-TimeSensorIntegrationTest] Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
2020-07-13 20:44:28,317 INFO org.quartz.core.QuartzScheduler [ScalaTest-main-running-TimeSensorIntegrationTest] Quartz Scheduler v.2.3.2 created.
2020-07-13 20:44:28,318 INFO org.quartz.simpl.RAMJobStore [ScalaTest-main-running-TimeSensorIntegrationTest] RAMJobStore initialized.
2020-07-13 20:44:28,319 INFO org.quartz.core.QuartzScheduler [ScalaTest-main-running-TimeSensorIntegrationTest] Scheduler meta-data: Quartz Scheduler (v2.3.2) 'za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler' with instanceId 'NON_CLUSTERED'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzSchedulerThreadPool' - with 0 threads.
  Using job-store 'org.quartz.simpl.RAMJobStore' - which does not support persistence. and is not clustered.

2020-07-13 20:44:28,319 INFO org.quartz.impl.StdSchedulerFactory [ScalaTest-main-running-TimeSensorIntegrationTest] Quartz scheduler 'za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler' initialized from an externally provided properties instance.
2020-07-13 20:44:28,319 INFO org.quartz.impl.StdSchedulerFactory [ScalaTest-main-running-TimeSensorIntegrationTest] Quartz scheduler version: 2.3.2
2020-07-13 20:44:28,319 INFO za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzSchedulerManager$ [ScalaTest-main-running-TimeSensorIntegrationTest] Starting Quartz Scheduler za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler now
2020-07-13 20:44:28,320 INFO org.quartz.core.QuartzScheduler [ScalaTest-main-running-TimeSensorIntegrationTest] Scheduler za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler_$_NON_CLUSTERED started.
2020-07-13 20:44:28,320 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [ScalaTest-main-running-TimeSensorIntegrationTest] Processing events. Sensors: Set()
2020-07-13 20:44:28,358 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-1-thread-6] Processing events successful
2020-07-13 20:44:30,009 INFO za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzJob [TimeSensorQuartz-thread-1] Starting Time Sensor Quartz Job
2020-07-13 20:44:30,013 DEBUG za.co.absa.hyperdrive.trigger.scheduler.eventProcessor.EventProcessor [TimeSensorQuartz-thread-1] Processing events. Sensor id: 1. Events: List(0)
2020-07-13 20:44:33,005 INFO za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzJob [TimeSensorQuartz-thread-2] Starting Time Sensor Quartz Job
2020-07-13 20:44:33,008 DEBUG za.co.absa.hyperdrive.trigger.scheduler.eventProcessor.EventProcessor [TimeSensorQuartz-thread-2] Processing events. Sensor id: 1. Events: List(0)
2020-07-13 20:44:33,365 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [ScalaTest-main-running-TimeSensorIntegrationTest] Processing events. Sensors: Set(1)
2020-07-13 20:44:33,372 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-1-thread-9] Processing events successful
2020-07-13 20:44:33,387 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [ScalaTest-main-running-TimeSensorIntegrationTest] Processing events. Sensors: Set(1)
2020-07-13 20:44:33,398 DEBUG za.co.absa.hyperdrive.trigger.scheduler.sensors.Sensors [pool-1-thread-13] Processing events successful
2020-07-13 20:44:33,402 INFO za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzSchedulerManager$ [ScalaTest-main-running-TimeSensorIntegrationTest] Stopping Quartz Scheduler za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler now
2020-07-13 20:44:33,402 INFO org.quartz.core.QuartzScheduler [ScalaTest-main-running-TimeSensorIntegrationTest] Scheduler za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler_$_NON_CLUSTERED paused.
2020-07-13 20:44:33,402 INFO za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzSchedulerManager$ [ScalaTest-main-running-TimeSensorIntegrationTest] Stopped Quartz Scheduler za.co.absa.hyperdrive.trigger.scheduler.sensors.time.TimeSensorQuartzScheduler
2020-07-13 20:44:33,673 INFO za.co.absa.hyperdrive.trigger.scheduler.executors.shell.ShellExecutor$ [Thread-5] echo
