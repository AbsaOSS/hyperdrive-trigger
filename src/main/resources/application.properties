server.port=7123
spring.resources.static-locations=classpath:/ui/
logging.path=logs
logging.file=${logging.path}/app.log

# How will users authenticate. Available options: inmemory, ldap
auth.mechanism=inmemory
# INMEMORY authentication: username and password defined here will be used for authentication.
auth.inmemory.user=user
auth.inmemory.password=password
# LDAP authentication: props template that has to be defined in case of LDAP authentication
#auth.ad.domain=
#auth.ad.server=
#auth.ldap.search.base=
#auth.ldap.search.filter=

appUniqueId=9c282190-4078-4380-8960-ce52f43b94fg

# Core properties.
# How many threads to use for each part of the "scheduler".
# Heart beat interval in milliseconds.
scheduler.thread.pool.size=10
scheduler.sensors.thread.pool.size=20
scheduler.executors.thread.pool.size=30
scheduler.jobs.parallel.number=4
scheduler.heart.beat=5000

kafkaSource.group.id=hyper_drive_${appUniqueId}
kafkaSource.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
kafkaSource.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
kafkaSource.poll.duration=500
kafkaSource.max.poll.records=3

#Spark yarn sink properties. Properties used to deploy and run Spark job in Yarn.
sparkYarnSink.hadoopResourceManagerUrlBase=
sparkYarnSink.hadoopConfDir=
sparkYarnSink.sparkHome=
sparkYarnSink.master=yarn
sparkYarnSink.submitTimeout=30000
sparkYarnSink.filesToDeploy=
sparkYarnSink.additionalConfs.spark.ui.port=4004
sparkYarnSink.additionalConfs.spark.driver.memory=1g
sparkYarnSink.additionalConfs.spark.executor.instances=3
sparkYarnSink.additionalConfs.spark.executor.cores=3
sparkYarnSink.additionalConfs.spark.executor.memory=2g

#Postgresql properties for connection to trigger metastore
db.driver=org.postgresql.Driver
db.url=jdbc:postgresql://
db.user=
db.password=
db.keepAliveConnection=true
db.connectionPool=HikariCP
db.numThreads=4